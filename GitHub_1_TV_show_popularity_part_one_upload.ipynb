{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial file upload: TMDB_tv_dataset_v3.csv"
      ],
      "metadata": {
        "id": "fZlUCRtcH96r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN_iaoT76TBx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(r\"TMDB_tv_dataset_v3.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SxsJyvJV9OF"
      },
      "outputs": [],
      "source": [
        "import matplotlib as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgAi5R50FiJg"
      },
      "source": [
        "# Step 1: Data Preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TXgCen6rJWm"
      },
      "outputs": [],
      "source": [
        "# To have an overview of the dataset\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To have an overview of missing data and types of data\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df.notnull().sum(),\n",
        "    'null_percent': df.isnull().mean() * 100,\n",
        "    'num_unique': df.nunique(),\n",
        "    'data_type': df.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "7NuiQixDECp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "_zA19Qp-EQxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDRQH24EtAFo"
      },
      "outputs": [],
      "source": [
        "# Show all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdmbMkl9cftB"
      },
      "source": [
        "## Look at rows and columns - Rows and columns duplication, check nulls and unknown - to\n",
        "\n",
        "---\n",
        "\n",
        "remove them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ9f2oPAiyEE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# To see columns with duplicate content - Didn't find any!\n",
        "\n",
        "duplicate_content = []\n",
        "\n",
        "for i in range(len(df.columns)):\n",
        "    for j in range(i+1, len(df.columns)):\n",
        "        col1 = df.columns[i]\n",
        "        col2 = df.columns[j]\n",
        "        if df[col1].equals(df[col2]):\n",
        "            duplicate_content.append((col1, col2))\n",
        "\n",
        "print(\"Columns with duplicate content:\", duplicate_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRVNYotC3J63"
      },
      "outputs": [],
      "source": [
        "# Screen for duplication whole row - by three parameters: id, name, original_name\n",
        "\n",
        "# ----------------------------\n",
        "# Part 1 – Checking and displaying duplicates\n",
        "# ----------------------------\n",
        "\n",
        "# 1️⃣ Finding all duplicate rows by specific columns\n",
        "duplicate_rows = df[df.duplicated(subset=['id', 'name', 'original_name'], keep=False)]\n",
        "\n",
        "# 2️⃣ Sorting the results to make it easy to see the duplicates\n",
        "duplicate_rows = duplicate_rows.sort_values(by=['id', 'name', 'original_name'])\n",
        "\n",
        "# 3️⃣ Displaying the first 10 examples of duplicates\n",
        "print(\"דוגמאות של כפילויות:\")\n",
        "print(duplicate_rows.head(10))\n",
        "\n",
        "# 4️⃣ Printing the number of rows Total that are identified as duplicates\n",
        "num_duplicates = len(duplicate_rows)\n",
        "print(f\"\\nנמצאו {num_duplicates} שורות כפולות (כולל כל העותקים).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkh3Rs0n1c1s"
      },
      "outputs": [],
      "source": [
        "# Part 2 – Remove all duplicates\n",
        "\n",
        "# Combine repeated rows into one - New df copy for this process: df_unique\n",
        "\n",
        "# Original number of rows\n",
        "original_rows = len(df)\n",
        "\n",
        "# ----------------------------\n",
        "# Remove all duplicates, keeping one instance for each combination of ['id', 'name', 'original_name']\n",
        "# ----------------------------\n",
        "df_unique = df.drop_duplicates(subset=['id', 'name', 'original_name']).reset_index(drop=True)\n",
        "\n",
        "# Calculate the number of non-unique rows removed\n",
        "removed_rows = original_rows - len(df_unique)\n",
        "\n",
        "print(\"\\nתוצאות לאחר הסרת כפילויות:\")\n",
        "print(df_unique.head())\n",
        "print(f\"סה\\\"כ שורות ייחודיות: {len(df_unique)}\")\n",
        "print(f\"סה\\\"כ שורות לא ייחודיות שנמחקו: {removed_rows}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw1ZOFp4BI00"
      },
      "source": [
        "# New data file name: df_unique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3y-cCjmxsZU"
      },
      "outputs": [],
      "source": [
        "print(f\"מספר העמודות: {len(df_unique.columns)}\")\n",
        "print(\"שמות העמודות:\")\n",
        "print(df_unique.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore target - popularity"
      ],
      "metadata": {
        "id": "--xH91qNE2NF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUvpFNGoTEj9"
      },
      "outputs": [],
      "source": [
        "df_unique['popularity'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=np.log1p(df['popularity']))\n",
        "plt.title('Boxplot of log(Popularity) - Horizontal', fontsize=14, weight='bold')\n",
        "plt.xlabel('log(Popularity + 1)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LsaTzhPtFEGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['popularity'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('Histogram of Popularity', fontsize=14, weight='bold')\n",
        "plt.xlabel('Popularity')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g7TNCzZNMIJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(np.log1p(df['popularity']), bins=30, kde=True, color='skyblue')\n",
        "plt.title('Histogram of log(Popularity + 1)', fontsize=14, weight='bold')\n",
        "plt.xlabel('log(Popularity + 1)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6-mXBMxVMOHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pop_max = df['popularity'].quantile(0.99)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df[df['popularity'] <= pop_max]['popularity'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('Histogram of Popularity (99th percentile)', fontsize=14, weight='bold')\n",
        "plt.xlabel('Popularity')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nPT3__4wMUlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOsBLwNvTewp"
      },
      "outputs": [],
      "source": [
        "# Check overall nulls\n",
        "import missingno as msno\n",
        "msno.matrix(df_unique)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Ir-CC6SThm"
      },
      "outputs": [],
      "source": [
        "# Checking nulls by percentage\n",
        "missing_percent = df_unique.isnull().mean() * 100\n",
        "print(missing_percent.sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pTsx6Gs1dC-"
      },
      "source": [
        "# Repeated columns with the same information\n",
        "## Columns handeling - unit or remove duplicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw4nRLp77vel"
      },
      "outputs": [],
      "source": [
        "# Repeated columns with the same information\n",
        "# Second - TV show name and original name\n",
        "\n",
        "print(df_unique[['name', 'original_name']].head(20))\n",
        "\n",
        "different_rows = df_unique[df_unique['name'] != df_unique['original_name']]\n",
        "print(different_rows[['name', 'original_name']].head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ybKVniG0RNC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Coulmns 'name' and 'original_name' - are mostly the same.\n",
        "# Check for missing values in 'name' column - without the name the row is useless\n",
        "\n",
        "\n",
        "missing_name_rows = df_unique[df_unique['name'].isnull()]\n",
        "missing_name_count = len(missing_name_rows)\n",
        "\n",
        "print(f\"מספר השורות שבהן 'name' חסר: {missing_name_count}\")\n",
        "missing_name_rows[['id', 'name', 'original_name']].head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbCsQrh7L-_6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a condition for true null in name:\n",
        "# 1. name is NaN\n",
        "# 2. name is empty (\"\")\n",
        "# 3. name contains only periods, such as \"...\" or \".....\"\n",
        "\n",
        "missing_name_condition = (\n",
        "    df_unique['name'].isna() |\n",
        "    (df_unique['name'].astype(str).str.strip() == \"\") |\n",
        "    (df_unique['name'].astype(str).str.match(r'^\\.*$'))\n",
        ")\n",
        "\n",
        "# Retrieve the missing rows according to the condition\n",
        "missing_name_rows = df_unique[missing_name_condition]\n",
        "\n",
        "print(f\"מספר השורות שבהן name חסר או בעייתי: {len(missing_name_rows)}\")\n",
        "missing_name_rows[['id', 'name', 'original_name']].head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvJ3NwLtNNov"
      },
      "outputs": [],
      "source": [
        "# Number of rows before deletion\n",
        "rows_before = len(df_unique)\n",
        "print(f\"סה\\\"כ שורות לפני המחיקה: {rows_before}\")\n",
        "\n",
        "# Condition for detecting invalid name: NaN, empty, or only dots\n",
        "missing_name_condition = (\n",
        "    df_unique['name'].isna() |\n",
        "    (df_unique['name'].astype(str).str.strip() == \"\") |\n",
        "    (df_unique['name'].astype(str).str.match(r'^\\.*$'))\n",
        ")\n",
        "\n",
        "# Saving rows for deletion (for viewing or saving)\n",
        "missing_name_rows = df_unique[missing_name_condition]\n",
        "deleted_ids = missing_name_rows['id'].tolist()\n",
        "\n",
        "# Deleting invalid rows\n",
        "df_unique = df_unique[~missing_name_condition].reset_index(drop=True)\n",
        "\n",
        "# Number of rows after Delete\n",
        "rows_after = len(df_unique)\n",
        "print(f\"סה\\\"כ שורות אחרי המחיקה: {rows_after}\")\n",
        "\n",
        "# How many rows were deleted\n",
        "removed_rows = rows_before - rows_after\n",
        "print(f\"סה\\\"כ שורות שנמחקו: {removed_rows}\")\n",
        "\n",
        "# Display the deleted IDs\n",
        "print(\"ID של השורות שנמחקו בגלל name לא תקין:\")\n",
        "print(deleted_ids)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSKK8Zra78Lc"
      },
      "outputs": [],
      "source": [
        "# Merge columns 'name' and 'original_name' into a new column called 'final_name' and delete the two original columns\n",
        "\n",
        "import re\n",
        "\n",
        "# --- Function to merge names ---\n",
        "def merge_names(row):\n",
        "    name = row['name']\n",
        "    original = row['original_name']\n",
        "\n",
        "    if pd.isna(name) and pd.isna(original):\n",
        "        return None\n",
        "    elif pd.isna(name):\n",
        "        return original\n",
        "    elif pd.isna(original):\n",
        "        return name\n",
        "    elif name == original:\n",
        "        return name\n",
        "    else:\n",
        "        return f\"{name} / {original}\"\n",
        "\n",
        "# --- Create the merged column ---\n",
        "df_unique['final_name'] = df_unique.apply(merge_names, axis=1)\n",
        "\n",
        "# --- Clean up and rebuild final_name ---\n",
        "df_unique['final_name'] = df_unique['final_name'].str.strip()                             # הסרת רווחים מיותרים\n",
        "df_unique['final_name'] = df_unique['final_name'].str.lower()                             # המרה לאותיות קטנות\n",
        "df_unique['final_name'] = df_unique['final_name'].str.replace(r'[^\\w\\s]', '', regex=True)  # הסרת תווים מיוחדים\n",
        "\n",
        "# --- Convert the column to pandas string type ---\n",
        "df_unique['final_name'] = df_unique['final_name'].astype('string')\n",
        "\n",
        "# --- Delete the original columns ---\n",
        "df_unique = df_unique.drop(columns=['name', 'original_name'])\n",
        "\n",
        "# --- Check the first 20 rows ---\n",
        "print(df_unique[['final_name']].head(20))\n",
        "\n",
        "# --- Check the data type ---\n",
        "print(\"\\nData type of final_name:\", df_unique['final_name'].dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27GSfuRGJvSK"
      },
      "source": [
        "# Repeated columns with the same information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm-SzgSKAxgX"
      },
      "outputs": [],
      "source": [
        "# Repeated columns with the same information\n",
        "# First - take a look at language\n",
        "\n",
        "print(df_unique[['original_language', 'languages', 'origin_country',\n",
        "                 'spoken_languages', 'production_countries']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtXjp-PBFsQP"
      },
      "outputs": [],
      "source": [
        "# Check language uniques\n",
        "language_columns = ['original_language', 'languages', 'origin_country',\n",
        "                    'spoken_languages', 'production_countries']\n",
        "\n",
        "for col in language_columns:\n",
        "    print(f\"\\n--- {col} ---\")\n",
        "    print(df_unique[col].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxN_HfQCEpWE"
      },
      "outputs": [],
      "source": [
        "### Remove column!!\n",
        "# Leave only one language column - \"original_language\", and remove the others.... Dont' remove the 'production_countries'\n",
        "\n",
        "cols_to_drop = ['languages', 'origin_country', 'spoken_languages']\n",
        "df_unique = df_unique.drop(columns=cols_to_drop)\n",
        "print(df_unique.columns)\n",
        "\n",
        "print(df_unique.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Columns with more then 50% missing values"
      ],
      "metadata": {
        "id": "agmx8XZZHHWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns with more then 50% missing data - nulls.\n",
        "\n",
        "# Calculate the percentage of missing rows in each column\n",
        "missing_percent = df_unique.isnull().mean() * 100\n",
        "\n",
        "# Select columns with 50% or more missing\n",
        "columns_50pct_or_more_missing = missing_percent[missing_percent >= 50].index.tolist()\n",
        "\n",
        "# Display the result\n",
        "print(\"עמודות עם יותר או שווה ל-50% ערכים חסרים:\")\n",
        "print(columns_50pct_or_more_missing)\n"
      ],
      "metadata": {
        "id": "qEIeoHrRHGYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show only the first 10 rows of columns with more than 50% missing\n",
        "# Calculate the percentage of missing rows in each column\n",
        "\n",
        "missing_percent = df_unique[columns_50pct_or_more_missing].isnull().mean() * 100\n",
        "\n",
        "# Create a new DataFrame with custom headers: column name + percentage of NULLs\n",
        "df_to_show = df_unique[columns_50pct_or_more_missing].head(10).copy()\n",
        "df_to_show.columns = [f\"{col} ({missing_percent[col]:.1f}% NULLs)\" for col in df_to_show.columns]\n",
        "\n",
        "# Show the first 10 rows with the new headers\n",
        "print(\"\\n10 השורות הראשונות מהעמודות עם יותר או שווה ל-50% ערכים חסרים (כולל אחוז NULLs):\")\n",
        "print(df_to_show)\n",
        "\n"
      ],
      "metadata": {
        "id": "alza6nqxHYy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove columns with more then 50% nulls, leave the 'production_countries' for furter analysis .\n",
        "\n",
        "\n",
        "# Calculate the percentage of missing rows in each column\n",
        "missing_percent = df_unique.isnull().mean() * 100\n",
        "\n",
        "# Select columns with 50% or more missing\n",
        "columns_50pct_or_more_missing = missing_percent[missing_percent >= 50].index.tolist()\n",
        "\n",
        "# Remove 'production_countries' from the list (do not delete it)\n",
        "columns_to_drop = [col for col in columns_50pct_or_more_missing if col != 'production_countries']\n",
        "\n",
        "# Drop columns\n",
        "df_unique = df_unique.drop(columns=columns_to_drop)\n",
        "\n",
        "# Display remaining columns\n",
        "print(\"העמודות אחרי המחיקה (כולל production_countries):\")\n",
        "print(df_unique.columns.tolist())\n"
      ],
      "metadata": {
        "id": "u1rwae1LHhmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check overall nulls\n",
        "import missingno as msno\n",
        "msno.matrix(df_unique)\n",
        "\n",
        "print(df_unique.columns)\n",
        "\n",
        "print(df_unique.shape)"
      ],
      "metadata": {
        "id": "i0Fk-57KHoiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCn8vJhPJ7Uu"
      },
      "source": [
        "# Deal with \"dates\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v6IyYakrh2h"
      },
      "outputs": [],
      "source": [
        "# Deal with \"dates\".\n",
        "\n",
        "# --- Convert date columns to datetime ---\n",
        "df_unique['first_air_date'] = pd.to_datetime(df_unique['first_air_date'], errors='coerce')\n",
        "df_unique['last_air_date'] = pd.to_datetime(df_unique['last_air_date'], errors='coerce')\n",
        "\n",
        "# --- Extract useful features ---\n",
        "\n",
        "# Year, month, day of the first date\n",
        "df_unique['first_year'] = df_unique['first_air_date'].dt.year\n",
        "df_unique['first_month'] = df_unique['first_air_date'].dt.month\n",
        "df_unique['first_day'] = df_unique['first_air_date'].dt.day\n",
        "\n",
        "# Year, month, day of the last date\n",
        "df_unique['last_year'] = df_unique['last_air_date'].dt.year\n",
        "df_unique['last_month'] = df_unique['last_air_date'].dt.month\n",
        "df_unique['last_day'] = df_unique['last_air_date'].dt.day\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6xEUsHRRwAB"
      },
      "outputs": [],
      "source": [
        "# For dates - make a new column\n",
        "# production_length\n",
        "# and leave only first_year and last_year\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Let's assume there are already NaNs instead of missing values\n",
        "# Calculating production_length with NaN\n",
        "df_unique['production_length'] = df_unique['last_year'] - df_unique['first_year']\n",
        "\n",
        "# Replacing all NaNs in production_length with -1\n",
        "df_unique['production_length'] = df_unique['production_length'].fillna(-1)\n",
        "\n",
        "# replace NaNs in the column headers if you want\n",
        "df_unique['first_year'] = df_unique['first_year'].fillna(-1)\n",
        "df_unique['last_year']  = df_unique['last_year'].fillna(-1)\n",
        "\n",
        "# Deleting unnecessary columns\n",
        "cols_to_drop = ['first_month', 'first_day', 'last_month', 'last_day', 'first_air_date', 'last_air_date']\n",
        "df_unique = df_unique.drop(columns=[col for col in cols_to_drop if col in df_unique.columns])\n",
        "\n",
        "# check\n",
        "df_unique[['first_year', 'last_year', 'production_length']].head(10)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDyrYbRASRAV"
      },
      "outputs": [],
      "source": [
        "msno.matrix(df_unique)\n",
        "df_unique.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove uninformative column 'poster_path'"
      ],
      "metadata": {
        "id": "hNPbcB8KILLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKeQbIKM9OXN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# A column that remains and is not informative, we will remove it\n",
        "# Deleting the column 'poster_path'\n",
        "df_unique = df_unique.drop(columns=['poster_path'])\n",
        "\n",
        "# Displaying the list of columns after deletion\n",
        "print(\"רשימת העמודות אחרי מחיקה:\")\n",
        "print(df_unique.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UifSIbk-sJ_a"
      },
      "outputs": [],
      "source": [
        "\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df_unique.notnull().sum(),\n",
        "    'null_percent': df_unique.isnull().mean() * 100,\n",
        "    'num_unique': df_unique.nunique(),\n",
        "    'data_type': df_unique.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4FPsf62cryI"
      },
      "source": [
        "# Try to nerrow uniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUDD5Hh4KOcK"
      },
      "outputs": [],
      "source": [
        "# Unique values ​​in each column\n",
        "# List of columns\n",
        "\n",
        "columns_to_check = ['type', 'in_production', 'genres', 'status', 'production_countries', 'networks', 'overview']\n",
        "\n",
        "# Ordered printing of unique values ​​with column name\n",
        "for col in columns_to_check:\n",
        "    print(f\"ערכים ייחודיים בעמודה '{col}':\")\n",
        "    print(df_unique[col].unique())\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Number of unique values\n",
        "print(\"Number of unique values in 'type':\", df_unique['type'].nunique())\n",
        "print(\"Number of unique values in 'in_production':\", df_unique['in_production'].nunique())\n",
        "print(\"Number of unique values in 'genres':\", df_unique['genres'].nunique())\n",
        "print(\"Number of unique values in 'status':\", df_unique['status'].nunique())\n",
        "print(\"Number of unique values in 'production_countries':\", df_unique['production_countries'].nunique())\n",
        "print(\"Number of unique values in 'networks':\", df_unique['networks'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdC-NMN6U3Ez"
      },
      "outputs": [],
      "source": [
        "# Show all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(df_unique.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5yRcOYtElDG"
      },
      "outputs": [],
      "source": [
        "# clean text for editing: remove spaces, capital letters...\n",
        "# Select all text columns\n",
        "text_cols = df_unique.select_dtypes(include=['object', 'string']).columns.tolist()\n",
        "print(\"עמודות טקסטואליות לניקוי:\", text_cols)\n",
        "\n",
        "# String cleaning function\n",
        "def clean_text(val):\n",
        "    if pd.isnull(val):\n",
        "        return val\n",
        "    val = str(val).strip().lower()\n",
        "    val = ','.join([v.strip() for v in val.split(',')])\n",
        "    return val\n",
        "\n",
        "# Remove extra commas and spaces and convert to lowercase for all text columns\n",
        "for col in text_cols:\n",
        "    df_unique[col] = df_unique[col].apply(clean_text)\n",
        "\n",
        "# Example display of the first 10 rows after cleaning\n",
        "print(df_unique[text_cols].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn1xCDCeX_RJ"
      },
      "outputs": [],
      "source": [
        "# Look for uniques in columns: 'production_countries' and 'networks'\n",
        "# Unique values ​​in each column\n",
        "\n",
        "print(df_unique['production_countries'].unique())\n",
        "print(df_unique['networks'].unique())\n",
        "\n",
        "# Number of unique values\n",
        "\n",
        "print(\"Number of unique values in 'production_countries':\", df_unique['production_countries'].nunique())\n",
        "print(\"Number of unique values in 'networks':\", df_unique['networks'].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSTYGfQTfxwx"
      },
      "outputs": [],
      "source": [
        "# Look for uniques in column: 'episode_run_time'\n",
        "\n",
        "df_unique['episode_run_time'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCRhEkkgiDoM"
      },
      "outputs": [],
      "source": [
        "# Max and Min uniques values in 'episode_run_time'\n",
        "\n",
        "unique_times = df_unique['episode_run_time'].dropna().unique()\n",
        "min_time = unique_times.min() if hasattr(unique_times, 'min') else min(unique_times)\n",
        "max_time = unique_times.max() if hasattr(unique_times, 'max') else max(unique_times)\n",
        "\n",
        "print(f\"Minimum episode run time: {min_time} minutes\")\n",
        "print(f\"Maximum episode run time: {max_time} minutes\")\n",
        "\n",
        "# zero values count\n",
        "num_zeros = (df_unique['episode_run_time'] == 0).sum()\n",
        "print(f\"Number of 0 values in episode_run_time: {num_zeros}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkcvqFk9x0-s"
      },
      "outputs": [],
      "source": [
        "# Check df\n",
        "summary = pd.DataFrame({\n",
        "    'null_percent': df_unique.isnull().mean() * 100,\n",
        "    'num_unique': df_unique.nunique(),\n",
        "    'data_type': df_unique.dtypes\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xDxqgv4yn65"
      },
      "source": [
        "## Changing strings/objects into category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGpFXafL0ix6"
      },
      "outputs": [],
      "source": [
        "# Lets start with the easy columns that have short limitied uniques\n",
        "categorical_cols = ['original_language', 'type', 'status', 'production_countries', 'networks', 'genres']\n",
        "for col in categorical_cols:\n",
        "    df_unique[col] = df_unique[col].astype('category')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_6fxfroAawx"
      },
      "outputs": [],
      "source": [
        "summary = pd.DataFrame({\n",
        "    'null_percent': df_unique.isnull().mean() * 100,\n",
        "    'num_unique': df_unique.nunique(),\n",
        "    'data_type': df_unique.dtypes\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxJSwzCdO_qi"
      },
      "outputs": [],
      "source": [
        "df_unique['production_countries'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7si7XdNRr8n"
      },
      "outputs": [],
      "source": [
        "# A better overview of 'production_countries' uniques\n",
        "\n",
        "# Allows printing all values ​​in columns without truncation\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Count the number of occurrences of each value in the production_countries column\n",
        "value_counts = df_unique['production_countries'].value_counts()\n",
        "\n",
        "# Turns this into a DataFrame for neat display\n",
        "value_counts_df = value_counts.reset_index()\n",
        "value_counts_df.columns = ['production_countries', 'count']\n",
        "\n",
        "# Displays all values ​​with the number of occurrences\n",
        "print(value_counts_df)\n",
        "\n",
        "# When finished, you can revert to the default settings\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGmOw0ieBAUV"
      },
      "outputs": [],
      "source": [
        "# Uniques in 'type'\n",
        "df_unique['type'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zowpMZzMHUF"
      },
      "outputs": [],
      "source": [
        "# Uniques in 'status'\n",
        "\n",
        "df_unique['status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEjQHM-aB34V"
      },
      "outputs": [],
      "source": [
        "# A better overview of 'original_language' uniques\n",
        "\n",
        "# Allows printing all values ​​in columns without truncation\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Displays all values ​​and frequencies\n",
        "print(df_unique['original_language'].value_counts())\n",
        "\n",
        "# When finished, you can return to the default settings\n",
        "# pd.reset_option('display.max_rows')\n",
        "# pd.reset_option('display.max_columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPCs2GMAZNt5"
      },
      "outputs": [],
      "source": [
        "# Unit 'original_language' uniques that are under 10 counts\n",
        "\n",
        "# Languages ​​under 10 became OTHER\n",
        "# For original_language\n",
        "\n",
        "\n",
        "# 1️⃣ Clean up spaces and extra characters in the language column\n",
        "df_unique['original_language'] = df_unique['original_language'].astype(str).str.strip().str.lower()\n",
        "\n",
        "# 2️⃣ Count all values ​​in the column\n",
        "language_counts = df_unique['original_language'].value_counts()\n",
        "\n",
        "# 3️⃣ Identify languages ​​that appear less than 10 times\n",
        "rare_languages = language_counts[language_counts < 10].index\n",
        "\n",
        "# 4️⃣ Replace rare values ​​with 'other' directly in the existing column\n",
        "df_unique['original_language'] = df_unique['original_language'].apply(\n",
        "    lambda x: 'other' if x in rare_languages else x\n",
        ")\n",
        "\n",
        "# 5️⃣ Displaying the results\n",
        "print(\"התפלגות השפות לאחר איחוד ערכים נדירים:\")\n",
        "print(df_unique['original_language'].value_counts().sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-roXx3CCLnR"
      },
      "outputs": [],
      "source": [
        "# A better overview of 'overview' uniques\n",
        "\n",
        "df_unique['overview'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldEE5tHoE4Wb"
      },
      "outputs": [],
      "source": [
        "# Clean text - spaces and other - in 'overview. For further editing\n",
        "df_unique['overview'] = df_unique['overview'].apply(\n",
        "    lambda x: x.strip().lower() if isinstance(x, str) else x\n",
        ")\n",
        "print(df_unique['overview'].head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmPu4dbAFyin"
      },
      "outputs": [],
      "source": [
        "# Check results\n",
        "df_unique['networks'].value_counts()\n",
        "print(df_unique['networks'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxGpBnhh-Wrw"
      },
      "outputs": [],
      "source": [
        "# # A better overview of 'networks' uniques\n",
        "\n",
        "# Allows printing all values ​​in columns without truncation\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Count the number of occurrences of each value in the networks column\n",
        "value_counts = df_unique['networks'].value_counts()\n",
        "\n",
        "# Convert to a DataFrame with columns: network name and number of occurrences\n",
        "value_counts_df = value_counts.reset_index()\n",
        "value_counts_df.columns = ['network', 'count']\n",
        "\n",
        "# Display all values ​​with number of occurrences\n",
        "print(value_counts_df)\n",
        "\n",
        "# When finished, you can revert to the default settings\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ9ZP1Fz_p65"
      },
      "outputs": [],
      "source": [
        "# Unit 'networks' uniques that are under 10 counts\n",
        "\n",
        "\n",
        "# 1️⃣ Count the number of occurrences of each network\n",
        "network_counts = df_unique['networks'].value_counts()\n",
        "\n",
        "# 2️⃣ Identify the values ​​that appear less than 10 times\n",
        "rare_networks = network_counts[network_counts < 10].index.tolist()\n",
        "\n",
        "# 3️⃣ Replace the rare values ​​with 'OTHER'\n",
        "df_unique['networks'] = df_unique['networks'].apply(lambda x: 'OTHER' if x in rare_networks else x)\n",
        "\n",
        "# 4️⃣ Check: Frequencies after merging\n",
        "print(\"שכיחויות הערכים לאחר איחוד נדירים ל-OTHER:\")\n",
        "print(df_unique['networks'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctN3BLIQF2OI"
      },
      "outputs": [],
      "source": [
        "# Explore 'genres' uniques\n",
        "\n",
        "df_unique['genres'].value_counts()\n",
        "print(df_unique['genres'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxn-hF8wG_Iq"
      },
      "outputs": [],
      "source": [
        "# explore 'genres' unique and clean spaces... for futher editing\n",
        "# Clean and merge genres and uniform description of words\n",
        "# Function to clean and sort genres\n",
        "\n",
        "def clean_genres(val):\n",
        "    if pd.isna(val) or val.strip() == '':\n",
        "        return val\n",
        "    items = [x.strip().lower() for x in val.split(',')]\n",
        "    items.sort()\n",
        "    return ','.join(items)\n",
        "\n",
        "# Apply the function to the entire existing column\n",
        "df_unique['genres'] = df_unique['genres'].apply(clean_genres)\n",
        "\n",
        "# Show the frequencies of the values ​​after cleaning\n",
        "print(df_unique['genres'].value_counts().head(100))\n",
        "\n",
        "# Summary: How many unique values ​​are there now In column\n",
        "num_unique_genres = df_unique['genres'].nunique()\n",
        "print(f\"\\nסה\\\"כ ערכים ייחודיים בעמודה 'genres' לאחר הניקוי: {num_unique_genres}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR_xbRer7Fyg"
      },
      "outputs": [],
      "source": [
        "# Unit 'genres' uniques that are under 10 counts\n",
        "\n",
        "# 1️⃣ Count all values ​​in the genres column\n",
        "counts = df_unique['genres'].value_counts()\n",
        "\n",
        "# 2️⃣ Identify values ​​that appear less than 10 times\n",
        "to_replace = counts[counts < 10].index.tolist()\n",
        "\n",
        "# 3️⃣ Replace rare values ​​with 'other'\n",
        "df_unique['genres'] = df_unique['genres'].apply(\n",
        "    lambda x: 'other' if x in to_replace else x\n",
        ")\n",
        "\n",
        "# 4️⃣ Test: Show the distribution after the union\n",
        "value_counts = df_unique['genres'].value_counts()\n",
        "print(value_counts)\n",
        "\n",
        "# 5️⃣ Show the summary of the number of unique values ​​after the union\n",
        "print(f\"\\nסה\\\"כ ערכים ייחודיים בעמודה 'genres' לאחר האיחוד: {df_unique['genres'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc6Eqk9D68ol"
      },
      "outputs": [],
      "source": [
        "# To have an overview of missing data and types of data\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df_unique.notnull().sum(),\n",
        "    'null_percent': df_unique.isnull().mean() * 100,\n",
        "    'num_unique': df_unique.nunique(),\n",
        "    'data_type': df_unique.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnyzDdlLJomP"
      },
      "outputs": [],
      "source": [
        "# List of columns to keep as object\n",
        "\n",
        "keep_object = ['overview', 'final_name']\n",
        "\n",
        "# Convert all other object columns to category\n",
        "for col in df_unique.select_dtypes(include=['object']).columns:\n",
        "    if col not in keep_object:\n",
        "        df_unique[col] = df_unique[col].astype('category')\n",
        "\n",
        "# Test\n",
        "# Create a comprehensive summary table for each column\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df_unique.notnull().sum(),\n",
        "    'null_percent': df_unique.isnull().mean() * 100,\n",
        "    'num_unique': df_unique.nunique(),\n",
        "    'data_type': df_unique.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Nb_td4MlUi"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD6rQzzmQSVg"
      },
      "outputs": [],
      "source": [
        "!pip install autoviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1bRuaQQR5bJ"
      },
      "outputs": [],
      "source": [
        "!pip install textblob\n",
        "!python -m textblob.download_corpora\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA reports"
      ],
      "metadata": {
        "id": "OtI94kptRYbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AutoViz report - Popularity as Target\n",
        "\n",
        "%matplotlib inline\n",
        "from autoviz.AutoViz_Class import AutoViz_Class\n",
        "\n",
        "AV = AutoViz_Class()\n",
        "\n",
        "df_auto = AV.AutoViz(\n",
        "    filename=\"\",\n",
        "    dfte=df_unique,\n",
        "    depVar=\"popularity\",\n",
        "    sep=\",\",\n",
        "    chart_format=\"png\",\n",
        "    max_rows_analyzed=30000,\n",
        "    verbose=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "fq1dFqWTTK4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ydata-profiling report\n",
        "\n",
        "!pip install ydata-profiling --quiet\n",
        "\n",
        "from ydata_profiling import ProfileReport\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KafU3JKoSXNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a report\n",
        "profile = ProfileReport(df_unique, title=\"EDA Report - Dataset Overview\", explorative=True)\n",
        "\n",
        "# For direct display in notebook (Jupyter / Colab)\n",
        "profile.to_notebook_iframe()\n",
        "\n",
        "# Or save to HTML file\n",
        "profile.to_file(\"EDA_report.html\")\n",
        "\n",
        "print(\"✅ דו\\\"ח EDA נוצר ונשמר כ-'EDA_report.html'\")\n"
      ],
      "metadata": {
        "id": "-YZsH9hRSgMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile.to_file(\"EDA_report.html\")"
      ],
      "metadata": {
        "id": "j8EGWauPSszu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "files.download(\"EDA_report.html\")"
      ],
      "metadata": {
        "id": "h1Sg5GjBSvBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPbwRZbfog2_"
      },
      "source": [
        "# Data cleansing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErRu-Td0okCD"
      },
      "outputs": [],
      "source": [
        "# Dealing with missing values\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df_unique.notnull().sum(),\n",
        "    'null_percent': df_unique.isnull().mean() * 100,\n",
        "    'num_unique': df_unique.nunique(),\n",
        "    'data_type': df_unique.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfzQdO6vz_Fq"
      },
      "outputs": [],
      "source": [
        "!pip install fancyimpute"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MICE - to fill missing values in 'overview'\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from fancyimpute import IterativeImputer\n",
        "\n",
        "# -----------------------------\n",
        "# 0️⃣ Filling missing values ​​in overview\n",
        "# -----------------------------\n",
        "# fill NaN\n",
        "df_unique['overview'] = df_unique['overview'].fillna(\"unknown\")\n",
        "# Remove spaces and convert to lowercase\n",
        "df_unique['overview'] = df_unique['overview'].apply(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
        "# Fill empty strings with \"unknown\"\n",
        "df_unique.loc[df_unique['overview'] == \"\", 'overview'] = \"unknown\"\n",
        "\n",
        "# -----------------------------\n",
        "# 1️⃣ Creating TF-IDF from overview (for MICE purpose only)\n",
        "# -----------------------------\n",
        "vectorizer = TfidfVectorizer(max_features=100)\n",
        "overview_tfidf = vectorizer.fit_transform(df_unique['overview']).toarray()\n",
        "overview_df = pd.DataFrame(overview_tfidf, columns=[f\"word_{i}\" for i in range(overview_tfidf.shape[1])])\n",
        "\n",
        "# -----------------------------\n",
        "# 2️⃣ Convert categories to numeric\n",
        "# -----------------------------\n",
        "categorical_cols = ['genres', 'networks', 'production_countries', 'type', 'status', 'adult']\n",
        "le_dict = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if pd.api.types.is_categorical_dtype(df_unique[col]):\n",
        "        df_unique[col] = df_unique[col].cat.add_categories([\"Unknown\"])\n",
        "    df_unique[col] = df_unique[col].fillna(\"Unknown\")\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    df_unique[col] = le.fit_transform(df_unique[col])\n",
        "    le_dict[col] = le\n",
        "\n",
        "# -----------------------------\n",
        "# 3️⃣ Preparing data for MICE (numeric only + TF-IDF)\n",
        "#    ✅ Removing id to avoid damaging MICE\n",
        "# -----------------------------\n",
        "numeric_cols = df_unique.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if col != 'id']  # <-- כאן הוספנו את התיקון\n",
        "\n",
        "df_for_mice = pd.concat([df_unique[numeric_cols], overview_df], axis=1)\n",
        "\n",
        "# -----------------------------\n",
        "# 4️⃣ Running MICE\n",
        "# -----------------------------\n",
        "imp = IterativeImputer(max_iter=10, random_state=0)\n",
        "df_filled_array = imp.fit_transform(df_for_mice)\n",
        "df_filled = pd.DataFrame(df_filled_array, columns=df_for_mice.columns)\n",
        "\n",
        "# -----------------------------\n",
        "# 5️⃣ Return categories to text mode\n",
        "# -----------------------------\n",
        "for col in categorical_cols:\n",
        "    le = le_dict[col]\n",
        "    df_filled[col] = df_filled[col].round().astype(int)\n",
        "    df_filled[col] = le.inverse_transform(df_filled[col])\n",
        "\n",
        "# -----------------------------\n",
        "# 6️⃣ Return the original overview (already filled with completions)\n",
        "# -----------------------------\n",
        "df_filled['overview'] = df_unique['overview']\n",
        "\n",
        "# -----------------------------\n",
        "# 7️⃣ Delete TF-IDF columns\n",
        "# -----------------------------\n",
        "tfidf_cols = [col for col in df_filled.columns if col.startswith(\"word_\")]\n",
        "df_filled.drop(columns=tfidf_cols, inplace=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 8️⃣ Return all original text columns that did not pass MICE\n",
        "# -----------------------------\n",
        "non_numeric_cols = df_unique.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
        "for col in non_numeric_cols:\n",
        "    if col != 'overview':\n",
        "        df_filled[col] = df_unique[col]\n",
        "\n",
        "# -----------------------------\n",
        "# ✅ Return id column as is\n",
        "# -----------------------------\n",
        "df_filled['id'] = df_unique['id']\n",
        "\n",
        "# -----------------------------\n",
        "# 9️⃣ check\n",
        "# -----------------------------\n",
        "print(\"מספר ערכים חסרים אחרי כל התהליך:\")\n",
        "print(df_filled.isnull().sum())\n"
      ],
      "metadata": {
        "id": "tQBublNhw3Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUgRSPl6p59w"
      },
      "source": [
        "# new dataframe - df_filled"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File check\n",
        "df_filled.info()"
      ],
      "metadata": {
        "id": "nia0g39rYXN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "df_filled.to_csv('df_new_GitHub.csv', index=False)\n",
        "files.download('df_new_GitHub.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "B_hWSssmC7ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continue in second file"
      ],
      "metadata": {
        "id": "7unU-tC9ZGOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For GitHub upload\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Lw8dx-2HkoxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "# Path ל-notebook הנוכחי\n",
        "path = '/content/drive/MyDrive/Project TV show popularity/advance project/More advanced project/For GitHub/Upload to GitHub/GitHub_1_TV_show_popularity_part_one_upload.ipynb'  # שנה לפי הנתיב שלך\n",
        "\n",
        "\n",
        "# קריאה ועריכה של ה-notebook\n",
        "nb = nbformat.read(path, as_version=4)\n",
        "\n",
        "# ניקוי metadata בעייתית\n",
        "if \"widgets\" in nb.metadata:\n",
        "    del nb.metadata[\"widgets\"]\n",
        "if \"colab\" in nb.metadata:\n",
        "    del nb.metadata[\"colab\"]\n",
        "if \"celltoolbar\" in nb.metadata:\n",
        "    del nb.metadata[\"celltoolbar\"]\n",
        "\n",
        "# שמירה מחדש\n",
        "nbformat.write(nb, path)\n",
        "print(\"✅ Notebook cleaned and ready for GitHub!\")"
      ],
      "metadata": {
        "id": "fEGwXlOMkvs0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}