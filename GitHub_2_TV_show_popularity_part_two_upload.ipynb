{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Second notebook - upload file df_new_GitHib"
      ],
      "metadata": {
        "id": "LwOTJFzZlrk2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9JiWcHTDMFv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('df_new_GitHub.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LNNzITmQAOr"
      },
      "outputs": [],
      "source": [
        "# To have an overview of the data\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df.notnull().sum(),\n",
        "    'null_percent': df.isnull().mean() * 100,\n",
        "    'num_unique': df.nunique(),\n",
        "    'data_type': df.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleansing - continue from the first file"
      ],
      "metadata": {
        "id": "BEwGFLVzzDUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove 'production_length' - no need in this analysis\n",
        "df = df.drop(columns=['production_length'])"
      ],
      "metadata": {
        "id": "0Hk_Otl5yiTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Last_year, first_year"
      ],
      "metadata": {
        "id": "_sXlkV-Q22ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore first and last_year contant - Any 0 or -1 will becoume 'knownown'\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "cols = ['first_year', 'last_year']\n",
        "\n",
        "for col in cols:\n",
        "    df[col] = df[col].apply(lambda x: \"Unknown\" if pd.isna(x) or x in [-1, 0, -1.0, 0.0] else str(int(x)))\n",
        "\n",
        "    num_unknown = (df[col] == \"Unknown\").sum()\n",
        "    percent_unknown = num_unknown / len(df) * 100\n",
        "    print(f\"[{col}] 'Unknown' values after conversion: {num_unknown} ({percent_unknown:.2f}%)\")\n",
        "\n",
        "# test columns\n",
        "print(\"\\nData types after conversion:\")\n",
        "print(df[cols].dtypes)\n"
      ],
      "metadata": {
        "id": "_hDWxgCd24OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check there are no 0 or -1\n",
        "cols = ['first_year', 'last_year']\n",
        "\n",
        "for col in cols:\n",
        "    num_neg1 = (df[col] == -1).sum()\n",
        "    print(f\"[{col}] Number of -1 values after conversion: {num_neg1}\")\n"
      ],
      "metadata": {
        "id": "KXEmC8Cc4FBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for future analysis - change unknown into NaN which is float64 or int64\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "cols = ['first_year', 'last_year']\n",
        "\n",
        "for col in cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df[col] = df[col].replace([-1, 0], np.nan)\n",
        "\n",
        "    # Statistics to check\n",
        "    num_missing = df[col].isna().sum()\n",
        "    percent_missing = num_missing / len(df) * 100\n",
        "    print(f\"[{col}] Missing/Unknown values after conversion: {num_missing} ({percent_missing:.2f}%)\")\n",
        "    print(f\"[{col}] Data type after conversion: {df[col].dtype}\")\n",
        "\n",
        "# check\n",
        "print(\"\\nSample data:\")\n",
        "print(df[cols].head())\n"
      ],
      "metadata": {
        "id": "o5mlgsG74zbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check high years to detect outlier years\n",
        "# Count table of values â€‹â€‹in column first_year\n",
        "\n",
        "counts = df['first_year'].value_counts()\n",
        "\n",
        "# 10 highest values\n",
        "top_10 = counts.nlargest(10)\n",
        "\n",
        "# 10 lowest values\n",
        "bottom_10 = counts.nsmallest(10)\n",
        "\n",
        "print(\"ğŸ”¹ 10 ×”×¢×¨×›×™× ×”×’×‘×•×”×™× ×‘×™×•×ª×¨:\")\n",
        "print(top_10)\n",
        "print(\"\\nğŸ”¹ 10 ×”×¢×¨×›×™× ×”× ××•×›×™× ×‘×™×•×ª×¨:\")\n",
        "print(bottom_10)"
      ],
      "metadata": {
        "id": "h2-pAzDj7lFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check 'first_year', 'last_year' columns after transformation\n",
        "ols = ['first_year', 'last_year']\n",
        "\n",
        "for col in cols:\n",
        "    print(f\"\\n[{col}] unique problematic values before conversion:\")\n",
        "    print(df[col].value_counts(dropna=False).head(10))"
      ],
      "metadata": {
        "id": "kCmF0Zrc7uoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outliers years need to be removed\n",
        "# Outliers years to remove\n",
        "\n",
        "remove_years = [2025, 2026, 2029, 2046]\n",
        "\n",
        "# Create a new DataFrame without the rows with these years\n",
        "df = df[~df['first_year'].isin(remove_years)]\n",
        "\n",
        "# check\n",
        "print(\"âœ… ×”×©×•×¨×•×ª ×¢× ×”×©× ×™× ×”×—×¨×™×’×•×ª ×”×•×¡×¨×•.\")\n",
        "print(df['first_year'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "MRBNhlZc8I3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check transformation\n",
        "# Allow all columns to be displayed\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# List of unrealistic years\n",
        "unrealistic_years = [2025, 2029, 2046, 1917, 1921, 1936, 1938, 1939, 1940, 1941, 1942, 1944, 1945]\n",
        "\n",
        "# Displays all rows that match the unrealistic years\n",
        "print(df[df['first_year'].isin(unrealistic_years)])"
      ],
      "metadata": {
        "id": "Mhj37Mnw8jZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check transformation\n",
        "# All unique values â€‹â€‹in column first_year\n",
        "\n",
        "unique_years = df['first_year'].unique()\n",
        "\n",
        "# Set aside the value -1 if it exists\n",
        "unique_years = unique_years[unique_years != -1]\n",
        "\n",
        "# Top 10 years\n",
        "top_10_years = np.sort(unique_years)[-10:]\n",
        "\n",
        "# Bottom 10 years\n",
        "bottom_10_years = np.sort(unique_years)[:10]\n",
        "\n",
        "print(\"ğŸ”¹ 10 ×”×©× ×™× ×”×’×‘×•×”×•×ª ×‘×™×•×ª×¨:\", top_10_years)\n",
        "print(\"ğŸ”¹ 10 ×”×©× ×™× ×”× ××•×›×•×ª ×‘×™×•×ª×¨:\", bottom_10_years)"
      ],
      "metadata": {
        "id": "y71x6JGv9lel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OUTLIERS WITH DATA CLEANSING"
      ],
      "metadata": {
        "id": "MH9en-KU7u4Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrMm_kmht9uf"
      },
      "source": [
        "### number_of_seasons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IVvY9kGOu7i"
      },
      "outputs": [],
      "source": [
        "# Explore uniques and distribution in 'number_of_seasons'\n",
        "df['number_of_seasons'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLnOhSIXk8Ch"
      },
      "outputs": [],
      "source": [
        "# More effective display\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1ï¸âƒ£ Enable full display of all rows in output\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# 2ï¸âƒ£ Full frequency table in order of values\n",
        "frequency_table = df['number_of_seasons'].value_counts().sort_index()\n",
        "print(\"Full frequency table:\")\n",
        "print(frequency_table)\n",
        "\n",
        "# 3ï¸âƒ£ List of all unique values\n",
        "unique_values = df['number_of_seasons'].unique().tolist()\n",
        "print(\"\\nList of unique values:\")\n",
        "print(unique_values)\n",
        "\n",
        "# 4ï¸âƒ£ Bar graph showing the distribution\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.barplot(x=frequency_table.index, y=frequency_table.values, palette=\"viridis\")\n",
        "plt.xlabel(\"Number of Seasons\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Number of Seasons\")\n",
        "plt.show()\n",
        "\n",
        "# 5ï¸âƒ£ Reset the option to display rows afterwards (optional)\n",
        "pd.reset_option('display.max_rows')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD8dSINjizAF"
      },
      "outputs": [],
      "source": [
        "# The smallest values â€‹â€‹in 'number_of_seasons'\n",
        "# 5 smallest values\n",
        "\n",
        "min_5_values = df['number_of_seasons'].nsmallest(5).unique()\n",
        "min_5_rows = df[df['number_of_seasons'].isin(min_5_values)]\n",
        "print(\"Rows with 5 smallest values of number_of_seasons:\")\n",
        "print(min_5_rows)\n",
        "\n",
        "# 5 greatest values\n",
        "max_10_values = df['number_of_seasons'].nlargest(10).unique()\n",
        "max_10_rows = df[df['number_of_seasons'].isin(max_10_values)]\n",
        "print(\"\\nRows with 10 largest values of number_of_seasons:\")\n",
        "print(max_10_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pMMjsm8qNuR"
      },
      "outputs": [],
      "source": [
        "# Data unknown and 0, -1 completion: Create new clean column 'number_of_seasons_clean'\n",
        "# Data completion by other columns such as\n",
        "# Your idea is to try to predict or \"complete\" the number_of_seasons value when it is unknown, based on other data we have such as:\n",
        "# number_of_episodes â†’ number of episodes\n",
        "# first_year / last_year â†’ years of broadcast\n",
        "# Maybe also episode_run_time, genres, type, etc.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create a copy of the original column\n",
        "df['number_of_seasons_clean'] = df['number_of_seasons'].copy()\n",
        "\n",
        "# 1ï¸âƒ£ Convert values â€‹â€‹0 and -1 to NaN (unknown)\n",
        "df['number_of_seasons_clean'] = df['number_of_seasons_clean'].replace([0, -1], np.nan)\n",
        "\n",
        "# 2ï¸âƒ£ Prepare variables for the model (numeric features only)\n",
        "features = ['number_of_episodes', 'episode_run_time', 'first_year', 'last_year']\n",
        "df_model = df[features + ['number_of_seasons_clean']].copy()\n",
        "\n",
        "# Keep known values â€‹â€‹and separate unknown\n",
        "train_data = df_model[df_model['number_of_seasons_clean'].notna()]\n",
        "predict_data = df_model[df_model['number_of_seasons_clean'].isna()]\n",
        "\n",
        "X_train = train_data[features]\n",
        "y_train = train_data['number_of_seasons_clean']\n",
        "\n",
        "X_predict = predict_data[features]\n",
        "\n",
        "# 3ï¸âƒ£ Handle missing values â€‹â€‹in features (NaN, -1 or 0)\n",
        "# First convert 0 and -1 in features For NaN\n",
        "X_train = X_train.replace([0, -1], np.nan)\n",
        "X_predict = X_predict.replace([0, -1], np.nan)\n",
        "\n",
        "# Fill the NaN with the median of each column\n",
        "X_train = X_train.fillna(X_train.median())\n",
        "X_predict = X_predict.fillna(X_train.median())\n",
        "\n",
        "# 4ï¸âƒ£ Create a regression model\n",
        "model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5ï¸âƒ£ Predict for the unknown only\n",
        "predicted_seasons = model.predict(X_predict)\n",
        "\n",
        "# 6ï¸âƒ£ Replace the values â€‹â€‹in the original df only for the unknown\n",
        "df.loc[df['number_of_seasons_clean'].isna(), 'number_of_seasons_clean'] = np.round(predicted_seasons).astype(int)\n",
        "\n",
        "# 7ï¸âƒ£ Check - How many values â€‹â€‹are filled\n",
        "print(\"Number of values filled:\", len(predicted_seasons))\n",
        "print(df['number_of_seasons_clean'].value_counts().sort_index())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data after transformation\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df.notnull().sum(),\n",
        "    'null_percent': df.isnull().mean() * 100,\n",
        "    'num_unique': df.nunique(),\n",
        "    'data_type': df.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "UNCEonXY7RZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After transformation - look at data\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assume you have the column number_of_seasons_clean\n",
        "\n",
        "# Filter values â€‹â€‹up to 10 (including NaN)\n",
        "seasons_cleaned = df['number_of_seasons_clean']\n",
        "seasons_cleaned_filtered = seasons_cleaned[ (seasons_cleaned <= 10) | (seasons_cleaned.isna()) ]\n",
        "\n",
        "# Count the number of occurrences of each unique value including NaN\n",
        "counts = seasons_cleaned_filtered.value_counts(dropna=False).sort_index(key=lambda x: [float('-inf') if pd.isna(v) else v for v in x])\n",
        "\n",
        "# Bar chart\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(counts.index.astype(str), counts.values, color='skyblue')  # ×”×¤×™×›×” ×œ-string ×›×“×™ ×©×”NaN ×™×•×¦×’\n",
        "plt.xlabel('Number of Seasons')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Number of Seasons (up to 10, including NaN)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Create a UNIQUE table with counts including NaN\n",
        "unique_table_seasons = pd.DataFrame({\n",
        "    'Number_of_Seasons': counts.index,\n",
        "    'Count': counts.values\n",
        "})\n",
        "\n",
        "# Display the full table\n",
        "print(unique_table_seasons)\n"
      ],
      "metadata": {
        "id": "WOpM87Due2Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swLHgWEAqVsF"
      },
      "outputs": [],
      "source": [
        "df['number_of_seasons_clean'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ4narIStzdC"
      },
      "outputs": [],
      "source": [
        "# Check for NaN in new column 'number_of_seasons_clean'\n",
        "# Check if they remain unknown (NaN)\n",
        "\n",
        "num_unknown = df['number_of_seasons_clean'].isna().sum()\n",
        "print(\"Number of remaining unknown values:\", num_unknown)\n",
        "\n",
        "if num_unknown > 0:\n",
        "    print(\"There are still unknown values remaining.\")\n",
        "else:\n",
        "    print(\"All unknown values have been filled.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IQR on 'number_of_seasons_clean'\n",
        "# Calculating Q1, Q3 and IQR\n",
        "\n",
        "Q1 = df['number_of_seasons_clean'].quantile(0.25)\n",
        "Q3 = df['number_of_seasons_clean'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Outliers limits\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Q1 = {Q1}, Q3 = {Q3}, IQR = {IQR}\")\n",
        "print(f\"Lower bound = {lower_bound}, Upper bound = {upper_bound}\")\n",
        "\n",
        "# Marking outliers **temporarily** without adding a new column\n",
        "outliers = df[(df['number_of_seasons_clean'] < lower_bound) | (df['number_of_seasons_clean'] > upper_bound)]\n",
        "\n",
        "# Summarize how many rows are marked as Outlier\n",
        "print(f\"Number of outliers: {len(outliers)}\")\n"
      ],
      "metadata": {
        "id": "RN6YasbMgILZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantile(0.95) - check values â€‹â€‹below to remove qutlires later\n",
        "# Choose the upper bound for the 95th percentile\n",
        "\n",
        "upper_percentile = 0.95\n",
        "upper_bound = df['number_of_seasons_clean'].quantile(upper_percentile)\n",
        "print(f\"95th percentile upper bound = {upper_bound}\")\n",
        "\n",
        "# Filter Outliers **temporarily**, without creating a new column\n",
        "outliers = df[df['number_of_seasons_clean'] > upper_bound]\n",
        "\n",
        "# Summarize how many rows are filtered as Outliers\n",
        "print(f\"Number of outliers (95th percentile): {len(outliers)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "guiYYYkdcKIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph of 'number_of_seasons_clean' outliers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample for a quick graph\n",
        "sample_df = df.sample(10000, random_state=42)\n",
        "\n",
        "# Outlier calculation according to the 95th percentile of number_of_seasons_clean\n",
        "upper_bound = df['number_of_seasons_clean'].quantile(0.95)\n",
        "sample_df['is_outlier'] = sample_df['number_of_seasons_clean'] > upper_bound\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "sns.scatterplot(\n",
        "    data=sample_df,\n",
        "    x='number_of_seasons_clean',\n",
        "    y='popularity',\n",
        "    hue='is_outlier',\n",
        "    palette={False:'blue', True:'red'},\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "plt.xlabel('Number of Seasons')\n",
        "plt.ylabel('Popularity')\n",
        "plt.title('Popularity vs Number of Seasons (Outliers >95th percentile in red)')\n",
        "plt.legend(title='Outlier (>95th percentile)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lj1p0EcpLnsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "cTxjxHFoLrno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove outliers here!!!!!!!!!!!\n",
        "# remove 95% top outliers number_of_seasons_clean !!!!!!!!!\n",
        "# Upper bound for 95th percentile\n",
        "\n",
        "upper_percentile = 0.95\n",
        "upper_bound = df['number_of_seasons_clean'].quantile(upper_percentile)\n",
        "print(f\"95th percentile upper bound = {upper_bound}\")\n",
        "\n",
        "# Keep only rows that are not Outliers\n",
        "df = df[df['number_of_seasons_clean'] <= upper_bound]\n",
        "\n",
        "# Check how many rows are left\n",
        "print(f\"Number of rows after removing outliers: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "T0ItvSjcdosl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove original 'number_of_seasons' column - no need for it anymore\n",
        "\n",
        "df = df.drop(columns=['number_of_seasons'])"
      ],
      "metadata": {
        "id": "pVb1IbyIhEOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jl6HuzXvQrY"
      },
      "source": [
        "### number_of_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNb_hNiDvWB5"
      },
      "outputs": [],
      "source": [
        "# 'number_of_episodes' uniques explore\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1ï¸âƒ£ Full frequency table for number_of_episodes\n",
        "pd.set_option('display.max_rows', None)\n",
        "frequency_table_episodes = df['number_of_episodes'].value_counts().sort_index()\n",
        "print(\"Full frequency table for number_of_episodes:\")\n",
        "print(frequency_table_episodes)\n",
        "\n",
        "# 2ï¸âƒ£ List of all unique values\n",
        "unique_values_episodes = df['number_of_episodes'].unique().tolist()\n",
        "print(\"\\nList of unique values for number_of_episodes:\")\n",
        "print(unique_values_episodes)\n",
        "\n",
        "# 3ï¸âƒ£ Reset the option to display rows afterwards (optional)\n",
        "pd.reset_option('display.max_rows')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data unknown and 0, -1 completion: Create new clean column 'number_of_episodes_clean'\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# ----- Step 1: Work on a copy to avoid damaging the original DF -----\n",
        "df_work = df.copy()\n",
        "\n",
        "# ----- Step 2: Create a clean column with unknown -----\n",
        "df_work['number_of_episodes_clean'] = df_work['number_of_episodes'].replace(0, np.nan)\n",
        "\n",
        "# ----- Step 3: Choose variables to predict -----\n",
        "features = ['number_of_seasons_clean', 'episode_run_time', 'first_year', 'last_year']\n",
        "\n",
        "# ----- Step 4: Handle missing values â€‹â€‹in features temporarily -----\n",
        "df_features = df_work[features].replace(-1, np.nan)\n",
        "\n",
        "# ----- Step 5: Identify rows with unknown -----\n",
        "predict_indexes = df_work[df_work['number_of_episodes_clean'].isna()].index\n",
        "\n",
        "# Predict only where there is at least some information\n",
        "valid_predict_indexes = df_features.loc[predict_indexes].dropna(how='all').index\n",
        "\n",
        "# ----- Step 6: Training data -----\n",
        "train_data = df_work[df_work['number_of_episodes_clean'].notna()]\n",
        "X_train = train_data[features].copy()\n",
        "X_train = X_train.replace(-1, np.nan).fillna(X_train.median())\n",
        "y_train = train_data['number_of_episodes_clean']\n",
        "\n",
        "# Prediction data\n",
        "X_predict = df_features.loc[valid_predict_indexes].fillna(X_train.median())\n",
        "\n",
        "# ----- Step 7: Random Forest model -----\n",
        "model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ----- Step 8: Predict -----\n",
        "predicted_values = np.round(model.predict(X_predict)).astype(int)\n",
        "\n",
        "# No 0 episodes â€“ all 0s become 1\n",
        "predicted_values[predicted_values < 1] = 1\n",
        "\n",
        "# ----- Step 9: Insert prediction into column -----\n",
        "df_work.loc[valid_predict_indexes, 'number_of_episodes_clean'] = predicted_values\n",
        "\n",
        "# ----- Step 10: Test -----\n",
        "print(\"âœ… ×—×™×–×•×™ ×”×•×©×œ× ×‘×”×¦×œ×—×”!\")\n",
        "print(\"ğŸ”¢ ××¡×¤×¨ ×¢×¨×›×™× ×©×”×•×©×œ××•:\", len(predicted_values))\n",
        "print(df_work['number_of_episodes_clean'].value_counts().sort_index())\n"
      ],
      "metadata": {
        "id": "FwtT_p3Z9KP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New df: df_work"
      ],
      "metadata": {
        "id": "tUolA3AHDYne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check df_work for 'number_of_episodes_clean' values\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# ----- Step 1: Counts -----\n",
        "counts = df_work['number_of_episodes_clean'].value_counts().sort_index()\n",
        "\n",
        "# ----- Step 2: Bar chart -----\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.bar(counts.index, counts.values, color='skyblue')\n",
        "plt.xlabel('Number of Episodes')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Number of Episodes (after filling unknowns)')\n",
        "plt.xticks(counts.index, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# ----- Step 3: UNIQUE table with counts -----\n",
        "unique_table = pd.DataFrame({\n",
        "    'Number_of_Episodes': counts.index,\n",
        "    'Count': counts.values\n",
        "})\n",
        "\n",
        "# Display the full table\n",
        "print(unique_table)\n"
      ],
      "metadata": {
        "id": "vhphEWUe9UhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 36 values in df_work 'number_of_episodes_clean'\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assume you have the column number_of_episodes_clean\n",
        "episodes_cleaned = df_work['number_of_episodes_clean']\n",
        "\n",
        "# Filter values â€‹â€‹up to 36 (including NaN)\n",
        "episodes_filtered = episodes_cleaned[ (episodes_cleaned <= 36) | (episodes_cleaned.isna()) ]\n",
        "\n",
        "# Count the number of occurrences of each unique value including NaN\n",
        "counts = episodes_filtered.value_counts(dropna=False).sort_index(key=lambda x: [float('-inf') if pd.isna(v) else v for v in x])\n",
        "\n",
        "# Bar chart\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.bar(counts.index.astype(str), counts.values, color='skyblue')  # ×”×¤×™×›×” ×œ-string ×›×“×™ ×©×”NaN ×™×•×¦×’\n",
        "plt.xlabel('Number of Episodes')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Number of Episodes (up to 36, including NaN)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Create a UNIQUE table with counts including NaN\n",
        "unique_table_episodes = pd.DataFrame({\n",
        "    'Number_of_Episodes': counts.index,\n",
        "    'Count': counts.values\n",
        "})\n",
        "\n",
        "# Display the full table\n",
        "print(unique_table_episodes)\n",
        "\n"
      ],
      "metadata": {
        "id": "qAHIPqWk24Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compared to original df file - # Display the first 36 values â€‹â€‹in df 'number_of_episodes'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assume you have the column number_of_episodes\n",
        "episodes_cleaned = df['number_of_episodes']\n",
        "\n",
        "# Filter values â€‹â€‹up to 36 (including NaN)\n",
        "episodes_filtered = episodes_cleaned[ (episodes_cleaned <= 36) | (episodes_cleaned.isna()) ]\n",
        "\n",
        "# Count the number of occurrences of each unique value including NaN\n",
        "counts = episodes_filtered.value_counts(dropna=False).sort_index(key=lambda x: [float('-inf') if pd.isna(v) else v for v in x])\n",
        "\n",
        "# Bar chart\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.bar(counts.index.astype(str), counts.values, color='skyblue')  # ×”×¤×™×›×” ×œ-string ×›×“×™ ×©×”NaN ×™×•×¦×’\n",
        "plt.xlabel('Number of Episodes')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Number of Episodes (up to 36, including NaN)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Create a UNIQUE table with counts including NaN\n",
        "unique_table_episodes = pd.DataFrame({\n",
        "    'Number_of_Episodes': counts.index,\n",
        "    'Count': counts.values\n",
        "})\n",
        "\n",
        "# Display the full table\n",
        "print(unique_table_episodes)\n"
      ],
      "metadata": {
        "id": "Ku58kJrL2YD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0CQldr7Pv03"
      },
      "outputs": [],
      "source": [
        "# Check df_work\n",
        "(df_work['number_of_episodes_clean'] == 0).sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To have an overview of missing data and types of data\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df_work.notnull().sum(),\n",
        "    'null_percent': df_work.isnull().mean() * 100,\n",
        "    'num_unique': df_work.nunique(),\n",
        "    'data_type': df_work.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "SFvIN0xY56zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantile(0.95) - check values â€‹â€‹below to remove outliers later\n",
        "\n",
        "# Variable to check\n",
        "\n",
        "col = 'number_of_episodes_clean'\n",
        "\n",
        "# Calculate the upper bound for the 95th percentile\n",
        "upper_bound = df_work[col].quantile(0.95)\n",
        "print(f\"95th percentile upper bound for {col}: {upper_bound}\")\n",
        "\n",
        "# Filter the rows above the limit\n",
        "df_work = df_work[df_work[col] <= upper_bound]\n",
        "\n",
        "# Check how many rows are left\n",
        "print(f\"Number of rows after removing 95th percentile outliers: {len(df_work)}\")\n"
      ],
      "metadata": {
        "id": "YsInJ-wIN7wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to remove seasons outliers!!!!!\n",
        "# remove outliers here!!!!!!!!!!!\n",
        "# remove 95% top outliers number_of_episodes_clean !!!!!!!!!\n",
        "# Variable to test\n",
        "\n",
        "col = 'number_of_episodes_clean'\n",
        "\n",
        "# Calculate the upper bound for the 95th percentile\n",
        "upper_bound = df_work[col].quantile(0.95)\n",
        "print(f\"95th percentile upper bound for {col}: {upper_bound}\")\n",
        "\n",
        "# Filter the rows above the limit\n",
        "df_filtered = df_work[df_work[col] <= upper_bound]\n",
        "\n",
        "# Output: Number of rows removed and percentage\n",
        "num_removed = len(df_work) - len(df_filtered)\n",
        "percent_removed = num_removed / len(df_work) * 100\n",
        "\n",
        "print(f\"Number of rows after removing 95th percentile outliers: {len(df_filtered)}\")\n",
        "print(f\"Number of rows removed: {num_removed}\")\n",
        "print(f\"Percent of dataset removed: {percent_removed:.2f}%\")\n",
        "\n",
        "# Save back to df_work\n",
        "df_work = df_filtered\n"
      ],
      "metadata": {
        "id": "epolBHg3ixtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete original 'number_of_seasons' column - no need from here\n",
        "\n",
        "df_work = df_work.drop(columns=['number_of_episodes'])"
      ],
      "metadata": {
        "id": "nQ6lEqo8hlws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_work.head()"
      ],
      "metadata": {
        "id": "7zXScLig_fua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_work.info()"
      ],
      "metadata": {
        "id": "CZg1rrCv_1lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JTvbwTgRaR5"
      },
      "outputs": [],
      "source": [
        "# new df\n",
        "df_clean = df_work.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New df name from here - df_clean"
      ],
      "metadata": {
        "id": "W2lRUFD0DvZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To have an overview of missing data and types of data\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df_clean.notnull().sum(),\n",
        "    'null_percent': df_clean.isnull().mean() * 100,\n",
        "    'num_unique': df_clean.nunique(),\n",
        "    'data_type': df_clean.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "Cak6stFHHLyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vote_count and vote_average"
      ],
      "metadata": {
        "id": "7sNmDzlsKIO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uniqes count for 'vote_count'\n",
        "df['vote_count'].value_counts().unique()"
      ],
      "metadata": {
        "id": "8C8NP3hCKcdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Better overlook of Uniqes count for 'vote_count'\n",
        "# with Graphic display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Count unique values\n",
        "vote_counts = df_clean['vote_count'].value_counts().sort_index()\n",
        "\n",
        "# Create a full table\n",
        "vote_counts_table = pd.DataFrame({\n",
        "    'Vote_Count': vote_counts.index,\n",
        "    'Frequency': vote_counts.values\n",
        "})\n",
        "\n",
        "# Display the full table\n",
        "print(vote_counts_table)\n",
        "\n",
        "# ----- Bar chart -----\n",
        "# Filter values â€‹â€‹up to 100 to make the graph readable\n",
        "\n",
        "vote_counts_filtered = vote_counts[vote_counts.index <= 100]\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.bar(vote_counts_filtered.index.astype(int), vote_counts_filtered.values, color='skyblue')\n",
        "plt.xlabel('Vote Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of vote_count (up to 100)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LVaedOZOMhz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Better overlook of Uniqes count for 'vote_average'\n",
        "# with Graphic display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Counting unique values\n",
        "vote_average = df_clean['vote_average'].value_counts().sort_index()\n",
        "\n",
        "# Creating a full table\n",
        "vote_average_table = pd.DataFrame({\n",
        "    'vote_average': vote_average.index,\n",
        "    'Frequency': vote_average.values\n",
        "})\n",
        "\n",
        "# Displaying the full table\n",
        "print(vote_average_table)\n",
        "\n",
        "# ----- Bar graph -----\n",
        "# Filtering values â€‹â€‹up to 100 to make the graph readable\n",
        "\n",
        "vote_average_filtered = vote_average[vote_average.index <= 100]\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.bar(vote_average_filtered.index.astype(int), vote_average_filtered.values, color='skyblue')\n",
        "plt.xlabel('Vote Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of vote_average (up to 100)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zeML-xusEjJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare values of 'vote_count' and 'vote_average'\n",
        "df_clean[['vote_count', 'vote_average']].head()\n",
        "df_clean[['vote_count', 'vote_average']].describe()"
      ],
      "metadata": {
        "id": "i7UZXLuMNiPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check correlation\n",
        "df_clean[['vote_count', 'vote_average']].corr()\n"
      ],
      "metadata": {
        "id": "AT_RSRtsNw95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check correlation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(df_clean['vote_count'], df_clean['vote_average'])\n",
        "plt.xlabel('Vote Count')\n",
        "plt.ylabel('Vote Average')\n",
        "plt.title('Vote Count vs Vote Average')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EXxs5TC-N7De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check correlation - log scale\n",
        "plt.scatter(df_clean['vote_count'], df_clean['vote_average'])\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Vote Count (log scale)')\n",
        "plt.ylabel('Vote Average')\n",
        "plt.title('Vote Count vs Vote Average (log scale)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PUyMRd_GOEvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check correlation - using bins\n",
        "\n",
        "df_clean['vote_count_group'] = pd.cut(df_clean['vote_count'], bins=[0, 10, 100, 1000, 10000, 100000])\n",
        "df_clean.groupby('vote_count_group')['vote_average'].mean()\n"
      ],
      "metadata": {
        "id": "SNAQNxorOSWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Pearson correlation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatterplot between vote_count and vote_average\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='vote_count', y='vote_average', data=df_clean, alpha=0.3)\n",
        "\n",
        "# Linear trendline\n",
        "sns.regplot(x='vote_count', y='vote_average', data=df_clean, scatter=False, color='red', line_kws={\"linewidth\":2})\n",
        "\n",
        "plt.xlabel('Vote Count')\n",
        "plt.ylabel('Vote Average')\n",
        "plt.title('Correlation between Vote Count and Vote Average')\n",
        "plt.xlim(0, 1000)\n",
        "plt.ylim(0, 10)\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation\n",
        "correlation = df_clean['vote_count'].corr(df_clean['vote_average'])\n",
        "print(f\"Pearson correlation between vote_count and vote_average: {correlation:.4f}\")\n"
      ],
      "metadata": {
        "id": "wSchrfT1NSg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new column for 'vote_count' with bins - 'vote_count_group'\n",
        "# Create a new binning column for vote_count\n",
        "\n",
        "def vote_binning(v):\n",
        "    if v == 0:\n",
        "        return \"No votes\"\n",
        "    elif v <= 10:\n",
        "        return \"Low\"\n",
        "    elif v <= 100:\n",
        "        return \"Medium\"\n",
        "    elif v <= 1000:\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"Very High\"\n",
        "\n",
        "df_clean['vote_count_group'] = df_clean['vote_count'].apply(vote_binning)\n",
        "\n",
        "# Check how many are in each group\n",
        "print(df_clean['vote_count_group'].value_counts())\n"
      ],
      "metadata": {
        "id": "IupJr8zMQylw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.info()"
      ],
      "metadata": {
        "id": "xe7HvHgjgT85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head()"
      ],
      "metadata": {
        "id": "IEnP8oM14emR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new column for 'vote_average' with bins - 'vote_average_group'\n",
        "\n",
        "# Creating a binning column for vote_average\n",
        "\n",
        "def vote_avg_binning(v):\n",
        "    if v < 3:\n",
        "        return \"Very Low\"\n",
        "    elif v < 5:\n",
        "        return \"Low\"\n",
        "    elif v < 7:\n",
        "        return \"Medium\"\n",
        "    elif v < 8.5:\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"Excellent\"\n",
        "\n",
        "df_clean['vote_average_group'] = df_clean['vote_average'].apply(vote_avg_binning)\n",
        "\n",
        "# Check how many there are in each group\n",
        "print(df_clean['vote_average_group'].value_counts())\n"
      ],
      "metadata": {
        "id": "S58yuBDnQ10K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(df_clean['vote_count_group'], df_clean['vote_average_group'])\n"
      ],
      "metadata": {
        "id": "p9L0f-_4Q8XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check correlation between the two new columns\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(\n",
        "    pd.crosstab(df_clean['vote_count_group'], df_clean['vote_average_group'], normalize='index'),\n",
        "    annot=True, fmt=\".2f\", cmap=\"Blues\"\n",
        ")\n",
        "plt.title(\"Vote Average Distribution by Vote Count Group\")\n",
        "plt.ylabel(\"Vote Count Group\")\n",
        "plt.xlabel(\"Vote Average Group\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ud2bsulURTEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.info()"
      ],
      "metadata": {
        "id": "mN5ngi0UiXdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check IQR outliers for vote_count\n",
        "# vote_count outliers\n",
        "# Variable to check\n",
        "\n",
        "col = 'vote_count'\n",
        "\n",
        "# Calculate Q1, Q3 and IQR\n",
        "Q1 = df_work[col].quantile(0.25)\n",
        "Q3 = df_work[col].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Outliers limits\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Q1 = {Q1}, Q3 = {Q3}, IQR = {IQR}\")\n",
        "print(f\"Lower bound = {lower_bound}, Upper bound = {upper_bound}\")\n",
        "\n",
        "# Mark the rows that are Outliers\n",
        "df_work['vote_count_outlier_iqr'] = (df_work[col] < lower_bound) | (df_work[col] > upper_bound)\n",
        "\n",
        "# Summary\n",
        "num_outliers = df_work['vote_count_outlier_iqr'].sum()\n",
        "percent_outliers = num_outliers / len(df_work) * 100\n",
        "print(f\"Number of outliers in {col}: {num_outliers}\")\n",
        "print(f\"Percent of dataset: {percent_outliers:.2f}%\")\n"
      ],
      "metadata": {
        "id": "EEuw5J8tkW4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check upper_percentile = 0.95 for 'vote_count'\n",
        "\n",
        "# Variable to test\n",
        "\n",
        "col = 'vote_count'\n",
        "\n",
        "# Calculation of the upper limit for the 95th percentile\n",
        "upper_percentile = 0.95\n",
        "upper_bound = df_work[col].quantile(upper_percentile)\n",
        "print(f\"95th percentile upper bound for {col} = {upper_bound}\")\n",
        "\n",
        "# Marking the outliers according to the 95th percentile\n",
        "df_work['vote_count_outlier_95'] = df_work[col] > upper_bound\n",
        "\n",
        "# Summary\n",
        "num_outliers = df_work['vote_count_outlier_95'].sum()\n",
        "percent_outliers = num_outliers / len(df_work) * 100\n",
        "print(f\"Number of outliers (95th percentile) in {col}: {num_outliers}\")\n",
        "print(f\"Percent of dataset: {percent_outliers:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Ptc3WJTCk4i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to remove vote_count outliers!!!!!\n",
        "# remove outliers here!!!!!!!!!!!\n",
        "# remove 95% top outliers vote_count !!!!!!!!!\n",
        "# Variable to test\n",
        "\n",
        "col = 'vote_count'\n",
        "\n",
        "# Calculate the upper bound for the 95th percentile\n",
        "upper_bound = df_work[col].quantile(0.95)\n",
        "print(f\"95th percentile upper bound for {col}: {upper_bound}\")\n",
        "\n",
        "# Filter the rows above the limit\n",
        "df_filtered = df_work[df_work[col] <= upper_bound]\n",
        "\n",
        "# Output: Number of rows removed and percentage\n",
        "num_removed = len(df_work) - len(df_filtered)\n",
        "percent_removed = num_removed / len(df_work) * 100\n",
        "\n",
        "print(f\"Number of rows after removing 95th percentile outliers: {len(df_filtered)}\")\n",
        "print(f\"Number of rows removed: {num_removed}\")\n",
        "print(f\"Percent of dataset removed: {percent_removed:.2f}%\")\n",
        "\n",
        "# Save back to df_work\n",
        "df_work = df_filtered\n"
      ],
      "metadata": {
        "id": "9hL8fDG3ofdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check upper_percentile = 0.95 for 'vote_average'\n",
        "\n",
        "# Variable to test\n",
        "col = 'vote_average'\n",
        "\n",
        "# # Setting a manual upper limit for Outliers\n",
        "upper_bound = 9\n",
        "\n",
        "# Mark Outliers\n",
        "df_work['vote_average_outlier_95'] = df_work[col] > upper_bound\n",
        "\n",
        "# Summary\n",
        "num_outliers = df_work['vote_average_outlier_95'].sum()\n",
        "percent_outliers = num_outliers / len(df_work) * 100\n",
        "\n",
        "print(f\"Upper bound for {col}: {upper_bound}\")\n",
        "print(f\"Number of outliers in {col} (95th percentile / >9): {num_outliers}\")\n",
        "print(f\"Percent of dataset: {percent_outliers:.2f}%\")\n"
      ],
      "metadata": {
        "id": "d9fil4G-mhl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to remove vote_average outliers!!!!!\n",
        "# remove outliers here!!!!!!!!!!!\n",
        "# remove 95% top outliers vote_average !!!!!!!!!\n",
        "# Variable to test\n",
        "\n",
        "col = 'vote_average'\n",
        "\n",
        "# Calculate the upper bound for the 95th percentile\n",
        "upper_bound = df_work[col].quantile(0.95)\n",
        "print(f\"95th percentile upper bound for {col}: {upper_bound}\")\n",
        "\n",
        "# Filter the rows above the limit\n",
        "df_filtered = df_work[df_work[col] <= upper_bound]\n",
        "\n",
        "# Output: Number of rows removed and percentage\n",
        "num_removed = len(df_work) - len(df_filtered)\n",
        "percent_removed = num_removed / len(df_work) * 100\n",
        "\n",
        "print(f\"Number of rows after removing 95th percentile outliers: {len(df_filtered)}\")\n",
        "print(f\"Number of rows removed: {num_removed}\")\n",
        "print(f\"Percent of dataset removed: {percent_removed:.2f}%\")\n",
        "\n",
        "# Save back to df_work\n",
        "df_work = df_filtered\n"
      ],
      "metadata": {
        "id": "ls29tRu2o18N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# explore episode_run_time"
      ],
      "metadata": {
        "id": "iLlIdgnaNmgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check 'episode_run_time'\n",
        "\n",
        "# Count the unique values â€‹â€‹in the episode_run_time column\n",
        "\n",
        "episode_run_time = df_clean['episode_run_time'].value_counts().sort_index()\n",
        "\n",
        "# Create a complete table with all values\n",
        "episode_run_time_table = pd.DataFrame({\n",
        "    'episode_run_time': episode_run_time.index,\n",
        "    'Frequency': episode_run_time.values\n",
        "})\n",
        "\n",
        "# Display the entire table\n",
        "pd.set_option('display.max_rows', None)  # ××’×“×™×¨ ×œ×”×¦×™×’ ××ª ×›×œ ×”×©×•×¨×•×ª\n",
        "print(episode_run_time_table)"
      ],
      "metadata": {
        "id": "wRo-LcR_KNxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.info()"
      ],
      "metadata": {
        "id": "3MZd4V3sgKIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data unknown and 0, -1 completion: Create new clean column 'episode_run_time_clean'\n",
        "# Label encoding to 'type' and 'genres' columns - create 'type_encoded' and 'genres_clean' new columns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# ----- Creating a clean target column -----\n",
        "df_clean['episode_run_time_clean'] = df_clean['episode_run_time'].replace(0, np.nan)\n",
        "\n",
        "# ----- LabelEncoding for column type -----\n",
        "le_type = LabelEncoder()\n",
        "df_clean['type_encoded'] = le_type.fit_transform(df_clean['type'])\n",
        "\n",
        "# ----- Multi-Label Encoding for the genres column -----\n",
        "df_clean['genres_clean'] = df_clean['genres'].apply(lambda x: x.split(',') if isinstance(x, str) else [])\n",
        "\n",
        "# Build a genre dictionary\n",
        "genre_to_id = {}\n",
        "current_id = 0\n",
        "for genres in df_clean['genres_clean']:\n",
        "    for g in genres:\n",
        "        if g not in genre_to_id:\n",
        "            genre_to_id[g] = current_id\n",
        "            current_id += 1\n",
        "\n",
        "# Convert lists to average genres\n",
        "df_clean['genres_encoded_avg'] = df_clean['genres_clean'].apply(\n",
        "    lambda lst: np.mean([genre_to_id[g] for g in lst]) if len(lst) > 0 else np.nan\n",
        ")\n",
        "\n",
        "# ----- Choose predictor variables -----\n",
        "features = ['number_of_seasons_clean', 'number_of_episodes_clean',\n",
        "            'first_year', 'last_year', 'type_encoded', 'genres_encoded_avg']\n",
        "\n",
        "# ----- Indexes for prediction (only where episode_run_time_clean is missing) -----\n",
        "predict_indexes = df_clean[df_clean['episode_run_time_clean'].isna()].index\n",
        "\n",
        "# ----- Training data -----\n",
        "train_data = df_clean[df_clean['episode_run_time_clean'].notna()]\n",
        "X_train = train_data[features].copy()\n",
        "y_train = train_data['episode_run_time_clean']\n",
        "\n",
        "# ----- Prediction data -----\n",
        "# Here we temporarily fill NaN only inside X_predict, not in the DF itself\n",
        "X_predict = df_clean.loc[predict_indexes, features].copy()\n",
        "X_predict_filled = X_predict.fillna(X_train.median())\n",
        "\n",
        "# ----- RandomForest model -----\n",
        "model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "model.fit(X_train.fillna(X_train.median()), y_train)\n",
        "\n",
        "# ----- Prediction -----\n",
        "predicted_values = np.round(model.predict(X_predict_filled)).astype(int)\n",
        "predicted_values[predicted_values < 1] = 1  # ××™×Ÿ 0 ×“×§×•×ª\n",
        "\n",
        "# ----- Insert prediction into target column only -----\n",
        "df_clean.loc[predict_indexes, 'episode_run_time_clean'] = predicted_values\n",
        "\n",
        "# ----- Check -----\n",
        "print(\"âœ… ×—×™×–×•×™ episode_run_time_clean ×”×•×©×œ×!\")\n",
        "print(\"ğŸ”¢ ××¡×¤×¨ ×¢×¨×›×™× ×©×”×•×©×œ××•:\", len(predicted_values))\n",
        "print(df_clean['episode_run_time_clean'].isna().sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "c6kX7CB7OopR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.info()"
      ],
      "metadata": {
        "id": "49FRyXA9gljl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare bettwen before anf after prediction\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution before completion (0 still exist)\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "(df['episode_run_time']\n",
        " .value_counts()\n",
        " .sort_index()\n",
        " .plot(kind='bar', color='skyblue'))\n",
        "plt.title(\"×”×ª×¤×œ×’×•×ª episode_run_time - ×œ×¤× ×™ ×”×”×©×œ××”\")\n",
        "plt.xlabel(\"Runtime (×“×§×•×ª)\")\n",
        "plt.ylabel(\"××¡×¤×¨ ×¡×“×¨×•×ª\")\n",
        "\n",
        "# Distribution after completion\n",
        "plt.subplot(1,2,2)\n",
        "(df_clean['episode_run_time_clean']\n",
        " .value_counts()\n",
        " .sort_index()\n",
        " .plot(kind='bar', color='orange'))\n",
        "plt.title(\"×”×ª×¤×œ×’×•×ª episode_run_time_clean - ××—×¨×™ ×”×”×©×œ××”\")\n",
        "plt.xlabel(\"Runtime (×“×§×•×ª)\")\n",
        "plt.ylabel(\"××¡×¤×¨ ×¡×“×¨×•×ª\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O41ZynqPQ0e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head()"
      ],
      "metadata": {
        "id": "nfeS55i14lC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# High resolution of episode_run_time_clean values â€‹â€‹- top 200 values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Take the first 200 values â€‹â€‹by runtime (sorted by index or value)\n",
        "top_200 = df_clean['episode_run_time_clean'].value_counts().sort_index().head(200)\n",
        "\n",
        "plt.figure(figsize=(16,6))\n",
        "top_200.plot(kind='bar', color='orange')\n",
        "plt.title(\"×”×ª×¤×œ×’×•×ª episode_run_time_clean - 200 ×”×¨××©×•× ×™× ××—×¨×™ ×”×”×©×œ××”\")\n",
        "plt.xlabel(\"Runtime (×“×§×•×ª)\")\n",
        "plt.ylabel(\"××¡×¤×¨ ×¡×“×¨×•×ª\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ip6nXyETRM05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values â€‹â€‹in episode_run_time_clean\n",
        "\n",
        "# Count unique values â€‹â€‹in episode_run_time column\n",
        "\n",
        "episode_run_time_clean = df_clean['episode_run_time_clean'].value_counts().sort_index()\n",
        "\n",
        "# Create a complete table with all values\n",
        "episode_run_time_table = pd.DataFrame({\n",
        "    'episode_run_time_clean': episode_run_time_clean.index,\n",
        "    'Frequency': episode_run_time_clean.values\n",
        "})\n",
        "\n",
        "# Display the entire table\n",
        "pd.set_option('display.max_rows', None)\n",
        "print(episode_run_time_table)"
      ],
      "metadata": {
        "id": "zchsIV7AWlXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the unique values â€‹â€‹in the episode_run_time column\n",
        "episode_run_time = df_clean['episode_run_time'].value_counts().sort_index()\n",
        "\n",
        "# Create a complete table with all the values\n",
        "episode_run_time_table = pd.DataFrame({\n",
        "    'episode_run_time': episode_run_time.index,\n",
        "    'Frequency': episode_run_time.values\n",
        "})\n",
        "\n",
        "# Display the entire table\n",
        "pd.set_option('display.max_rows', None)\n",
        "print(episode_run_time_table)"
      ],
      "metadata": {
        "id": "JMGWvbcIyAJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All unique values â€‹â€‹in the 'genres_encoded_avg' column\n",
        "\n",
        "unique_values = df_clean['genres_encoded_avg'].unique()\n",
        "print(unique_values)\n",
        "print(\"××¡×¤×¨ ×¢×¨×›×™× ×™×™×—×•×“×™×™×:\", len(unique_values))\n"
      ],
      "metadata": {
        "id": "N5nxhazkXVy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IQR analysis for 'episode_run_time_clean'\n",
        "col = 'episode_run_time_clean'\n",
        "\n",
        "# Calculation of Q1, Q3 and the IQR\n",
        "Q1 = df_clean[col].quantile(0.25)\n",
        "Q3 = df_clean[col].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Boundaries\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Mark Outliers\n",
        "df_clean['episode_run_time_outlier_iqr'] = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)\n",
        "\n",
        "# Summary\n",
        "num_outliers_iqr = df_clean['episode_run_time_outlier_iqr'].sum()\n",
        "percent_outliers_iqr = num_outliers_iqr / len(df_clean) * 100\n",
        "\n",
        "print(f\"IQR bounds: {lower_bound} â€“ {upper_bound}\")\n",
        "print(f\"Number of outliers (IQR): {num_outliers_iqr}\")\n",
        "print(f\"Percent of dataset: {percent_outliers_iqr:.2f}%\")\n"
      ],
      "metadata": {
        "id": "RtD9rRYxpj_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to remove episode_run_time_clean outliers!!!!!\n",
        "# remove outliers here!!!!!!!!!!!\n",
        "# remove IQR top outliers episode_run_time_clean !!!!!!!!!\n",
        "# Variable to test\n",
        "\n",
        "col = 'episode_run_time_clean'\n",
        "\n",
        "# Calculate Q1, Q3 and IQR\n",
        "Q1 = df_clean[col].quantile(0.25)\n",
        "Q3 = df_clean[col].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"IQR bounds: {lower_bound} â€“ {upper_bound}\")\n",
        "\n",
        "# Filter out rows that are not Outliers\n",
        "df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
        "\n",
        "# Check how many rows are left\n",
        "num_rows = len(df_clean)\n",
        "print(f\"Number of rows after removing IQR outliers: {num_rows}\")\n"
      ],
      "metadata": {
        "id": "BLKl1ADQrJ0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete original column 'episode_run_time' - no need for it anymore\n",
        "\n",
        "df_clean = df_clean.drop(columns=['episode_run_time'])"
      ],
      "metadata": {
        "id": "xo2Ob3_Gy09c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove 'genres' columns - no need anymore\n",
        "\n",
        "df_clean = df_clean.drop(columns=['genres'])"
      ],
      "metadata": {
        "id": "j9iIVANwxSxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continue outliers"
      ],
      "metadata": {
        "id": "znoprOUyN1n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After removing outliers above - this section will check if a second outlier removal is needed after Log transformation"
      ],
      "metadata": {
        "id": "w4sFFjXOTrGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check numeric columns for more deep outlier analysis\n",
        "\n",
        "numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns\n",
        "print(numeric_cols)"
      ],
      "metadata": {
        "id": "tP0q-JfQQUCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skewness check\n",
        "skew_values = df_clean[numeric_cols].skew().sort_values(ascending=False)\n",
        "print(skew_values)"
      ],
      "metadata": {
        "id": "h5kBTmFrQXKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before Log transformation for numeric columns - Check values â€‹â€‹under -1 or = -1.\n",
        "# Important for Log transformation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Numeric columns that fit the log, without target and id\n",
        "log_cols_features = [\n",
        "    'number_of_episodes_clean',\n",
        "    'episode_run_time_clean',\n",
        "    'number_of_seasons_clean',\n",
        "    'vote_count'\n",
        "]\n",
        "\n",
        "\n",
        "(df_clean[log_cols_features] <= -1).sum()"
      ],
      "metadata": {
        "id": "qG21qF9wQ8H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log transformation for these columns\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "log_cols_features = [\n",
        "    'number_of_episodes_clean',\n",
        "    'episode_run_time_clean',\n",
        "    'number_of_seasons_clean',\n",
        "    'vote_count'\n",
        "]\n",
        "\n",
        "continuous_df_log = np.log1p(df_clean[log_cols_features])"
      ],
      "metadata": {
        "id": "EUaLCFAnSHkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check IQR - this time after Log transformation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# ---- 1ï¸âƒ£ Setting up the columns for log and preparing ----\n",
        "log_cols_features = [\n",
        "    'number_of_episodes_clean',\n",
        "    'episode_run_time_clean',\n",
        "    'number_of_seasons_clean',\n",
        "    'vote_count'\n",
        "]\n",
        "\n",
        "# Handling negative or too small values\n",
        "df_clean[log_cols_features] = df_clean[log_cols_features].clip(lower=0)\n",
        "\n",
        "# Creating a log version\n",
        "continuous_df_log = np.log1p(df_clean[log_cols_features])\n",
        "\n",
        "# ---- 2ï¸âƒ£ Function to detect outliers by IQR ----\n",
        "def treat_outliers(df, features):\n",
        "    outliers_df = pd.DataFrame(index=df.index)\n",
        "    summary = {}\n",
        "\n",
        "    for feature in features:\n",
        "        Q1 = df[feature].quantile(0.25)\n",
        "        Q3 = df[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        outliers = ((df[feature] < lower_bound) | (df[feature] > upper_bound))\n",
        "        outliers_df[feature] = outliers.astype(int)\n",
        "        summary[feature] = {\n",
        "            \"lower_bound\": lower_bound,\n",
        "            \"upper_bound\": upper_bound,\n",
        "            \"n_outliers\": outliers.sum(),\n",
        "            \"percent_outliers\": (outliers.sum() / len(df)) * 100\n",
        "        }\n",
        "\n",
        "    summary_df = pd.DataFrame(summary).T.sort_values(by=\"percent_outliers\", ascending=False)\n",
        "    return outliers_df, summary_df\n",
        "\n",
        "# ---- 3ï¸âƒ£ identifying outliers ----\n",
        "outliers_df, summary_df = treat_outliers(continuous_df_log, continuous_df_log.columns)\n",
        "display(summary_df)\n",
        "\n",
        "# ---- 4ï¸âƒ£ cut of exceptions according to the 95th percentile ----\n",
        "for feature in continuous_df_log.columns:\n",
        "    cap = continuous_df_log[feature].quantile(0.95)\n",
        "    continuous_df_log[feature] = continuous_df_log[feature].clip(upper=cap)\n",
        "\n",
        "# ---- 5ï¸âƒ£ graph top 5 features with the most outliers ----\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(\n",
        "    x=summary_df.index[:5],\n",
        "    y=summary_df[\"percent_outliers\"][:5]\n",
        ")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top 5 Features with Highest Percent of Outliers\")\n",
        "plt.ylabel(\"Percent of Outliers\")\n",
        "plt.show()\n",
        "\n",
        "# ---- 6ï¸âƒ£ Testing of change in distribution and correlation with Target ----\n",
        "out_df = pd.DataFrame()\n",
        "\n",
        "for col in outliers_df:\n",
        "    out = continuous_df_log[col]\n",
        "    non_out = continuous_df_log[col][outliers_df[col] == 0]\n",
        "    cor_out = df_clean['popularity']\n",
        "    cor_non_out = df_clean['popularity'][outliers_df[col] == 0]\n",
        "\n",
        "    # Filter out NaN values for correlation calculation\n",
        "    valid_out = ~np.isnan(out) & ~np.isnan(cor_out)\n",
        "    valid_non_out = ~np.isnan(non_out) & ~np.isnan(cor_non_out)\n",
        "\n",
        "    if valid_out.sum() > 1 and valid_non_out.sum() > 1:\n",
        "        cor_change = '+' if abs(np.corrcoef(out[valid_out], cor_out[valid_out])[0, 1] -\n",
        "                                np.corrcoef(non_out[valid_non_out], cor_non_out[valid_non_out])[0, 1]) > 0.05 else '-'\n",
        "    else:\n",
        "        cor_change = '-'\n",
        "\n",
        "    dist_change = '+' if ks_2samp(out.dropna(), non_out.dropna())[1] < 0.05 else '-'\n",
        "\n",
        "    new_row = pd.DataFrame({'feature': [col],\n",
        "                            'outliers_cnt': [outliers_df[col].sum()],\n",
        "                            'distribution_changed': [dist_change],\n",
        "                            'correlation_changed': [cor_change]})\n",
        "\n",
        "    out_df = pd.concat([out_df, new_row], ignore_index=True)\n",
        "\n",
        "display(out_df)\n"
      ],
      "metadata": {
        "id": "P-vm-YHzT3FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add recommendation to remove or not outliers values\n",
        "# Add recommendation to consider drop\n",
        "\n",
        "\n",
        "out_df['drop'] = np.where(\n",
        "    (out_df['distribution_changed'] == '+') & (out_df['correlation_changed'] == '-'),\n",
        "    'yes',\n",
        "    'no'\n",
        ")\n",
        "\n",
        "# List of features to consider dropping or handling\n",
        "to_drop = out_df[out_df['drop'] == 'yes']['feature']\n",
        "\n",
        "# Display DataFrame\n",
        "display(out_df)\n"
      ],
      "metadata": {
        "id": "2RJLUT_FUEpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to tuples - 'tuples' column\n",
        "\n",
        "# Convert lists or tuples to strings\n",
        "for col in ['genres_clean']:\n",
        "    df_clean[col] = df_clean[col].apply(lambda x: str(x) if isinstance(x, (list, tuple)) else x)\n",
        "\n",
        "# Now you can convert to category without error\n",
        "categorical_cols = [\n",
        "    'type', 'status', 'networks', 'production_countries',\n",
        "    'original_language', 'vote_count_group', 'vote_average_group',\n",
        "    'type_encoded', 'genres_clean'\n",
        "]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    df_clean[col] = df_clean[col].astype('category')"
      ],
      "metadata": {
        "id": "RpTrhgC_eBgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deal outliers with CAPPING and show before and after graph\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ---- 0ï¸âƒ£ Convert categorical columns to category ----\n",
        "categorical_cols = [\n",
        "    'status', 'networks',\n",
        "    'original_language', 'vote_count_group', 'vote_average_group',\n",
        "    'type_encoded', 'genres_clean'\n",
        "]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    df_clean[col] = df_clean[col].astype('category')\n",
        "\n",
        "# ---- 1ï¸âƒ£ Set columns to log ----\n",
        "log_cols_features = [\n",
        "    'number_of_episodes_clean',\n",
        "    'episode_run_time_clean',\n",
        "    'number_of_seasons_clean',\n",
        "    'vote_count'\n",
        "]\n",
        "\n",
        "# Handling negative or small values Too\n",
        "df_clean[log_cols_features] = df_clean[log_cols_features].clip(lower=0)\n",
        "\n",
        "# Create initial log version\n",
        "continuous_df_log = np.log1p(df_clean[log_cols_features])\n",
        "\n",
        "# ---- 2ï¸âƒ£ List of features to be treated (drop == 'yes') ----\n",
        "features_to_cap = out_df[out_df['drop'] == 'yes']['feature'].tolist()\n",
        "\n",
        "# ---- 3ï¸âƒ£ Save log version before Capping ----\n",
        "continuous_df_log_before = continuous_df_log.copy()\n",
        "\n",
        "# ---- 4ï¸âƒ£ Cut to 95th percentile for these features ----\n",
        "for feature in features_to_cap:\n",
        "    cap = continuous_df_log[feature].quantile(0.95)\n",
        "    continuous_df_log[feature] = continuous_df_log[feature].clip(upper=cap)\n",
        "\n",
        "# ---- 5ï¸âƒ£ Comparison graph before and after ----\n",
        "for feature in features_to_cap:\n",
        "    plt.figure(figsize=(10,4))\n",
        "    sns.kdeplot(continuous_df_log_before[feature], label='Before capping', fill=True)\n",
        "    sns.kdeplot(continuous_df_log[feature], label='After capping', fill=True)\n",
        "    plt.title(f'Distribution of {feature} Before/After Capping')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# ---- 6ï¸âƒ£ final result ----\n",
        "print(\"Features capped at 95th percentile:\", features_to_cap)\n",
        "display(continuous_df_log.head())\n"
      ],
      "metadata": {
        "id": "Z_c4va5tcaGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overall view after and before CAPPING - outliers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# List of all the features for the plot\n",
        "all_features = ['number_of_episodes_clean', 'episode_run_time_clean',\n",
        "                'number_of_seasons_clean', 'vote_count']\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "for feature in all_features:\n",
        "    sns.kdeplot(continuous_df_log_before[feature], label=f'{feature} Before', linestyle='--')\n",
        "    sns.kdeplot(continuous_df_log[feature], label=f'{feature} After', linestyle='-')\n",
        "\n",
        "plt.title(\"Comparison of Distributions Before/After Capping for All Features\")\n",
        "plt.xlabel(\"Log1p Values\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "heKFcdmJgAvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.info()"
      ],
      "metadata": {
        "id": "s0Fgx2rs57zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head()"
      ],
      "metadata": {
        "id": "hIvmQ8PRgOa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target Encoding"
      ],
      "metadata": {
        "id": "Pdtc6iQA0WsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df_clean.notnull().sum(),\n",
        "    'null_percent': df_clean.isnull().mean() * 100,\n",
        "    'num_unique': df_clean.nunique(),\n",
        "    'data_type': df_clean.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "zSTLxrSc0vNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding to continue analysis\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1ï¸âƒ£ Clean copy of the DataFrame ---\n",
        "df_final = df_clean.copy()\n",
        "\n",
        "# --- 2ï¸âƒ£ Keep numeric features as they are (or with log if you wanted) ---\n",
        "numeric_features_continuous = list(continuous_df_log.columns)\n",
        "for col in numeric_features_continuous:\n",
        "    df_final[col] = continuous_df_log[col]\n",
        "\n",
        "# --- 3ï¸âƒ£ categorical features with Label Encoding ---\n",
        "categorical_cols = [\n",
        "    'type', 'status', 'genres_clean', 'networks',\n",
        "    'original_language', 'vote_count_group', 'vote_average_group',\n",
        "    'production_countries'\n",
        "]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_final[col] = le.fit_transform(df_final[col].astype(str))\n",
        "\n",
        "# --- 4ï¸âƒ£ Boolean columns remain as Bool ---\n",
        "bool_cols = ['adult', 'in_production', 'year_known']\n",
        "\n",
        "# --- 5ï¸âƒ£ list of feature sets ---\n",
        "numeric_features = numeric_features_continuous + ['vote_average', 'first_year', 'last_year',\n",
        "                                                  'genres_encoded_avg', 'type_encoded']\n",
        "categorical_features = categorical_cols + bool_cols\n",
        "target = 'popularity'\n",
        "\n",
        "# --- 6ï¸âƒ£ test ---\n",
        "print(\"Columns ready for modeling:\")\n",
        "display(df_final.head())\n",
        "\n",
        "print(\"Numeric features:\", numeric_features)\n",
        "print(\"Categorical features (Label Encoded + Bool):\", categorical_features)\n",
        "print(\"Target:\", target)\n"
      ],
      "metadata": {
        "id": "UdBTJ71b68sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df_final.notnull().sum(),\n",
        "    'null_percent': df_final.isnull().mean() * 100,\n",
        "    'num_unique': df_final.nunique(),\n",
        "    'data_type': df_final.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "tn_khe940t_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check correlation again - for all current columns\n"
      ],
      "metadata": {
        "id": "8QqbJlLHz8R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation with 'popularity' - the target column\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- 1ï¸âƒ£ select only numeric columns including target ----\n",
        "numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "# ---- 2ï¸âƒ£ calculate correlation ----\n",
        "corr_matrix = df_clean[numeric_cols].corr()\n",
        "\n",
        "# ---- 3ï¸âƒ£ remove only the line of popularity ----\n",
        "popularity_corr = corr_matrix['popularity'].sort_values(ascending=False)\n",
        "\n",
        "print(\"Correlation of all numeric features with popularity:\")\n",
        "display(popularity_corr)\n",
        "\n",
        "# ---- 4ï¸âƒ£ heatmap of correlation with popularity ----\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(popularity_corr.to_frame(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title(\"Correlation with Popularity\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "clRWPUc_gkjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deal with tuples before analyzing important words for popularity score\n",
        "\n",
        "# Identifies columns that have lists or tuples\n",
        "\n",
        "list_cols = [col for col in df_clean.columns\n",
        "             if df_clean[col].apply(lambda x: isinstance(x, (list, tuple))).any()]\n",
        "\n",
        "print(\"Columns with list/tuple values:\", list_cols)\n",
        "\n",
        "# Convert each list/tuple to a string (comma-separated)\n",
        "for col in list_cols:\n",
        "    df_clean[col] = df_clean[col].apply(lambda x: ','.join(x) if isinstance(x, (list, tuple)) else str(x))\n",
        "\n",
        "# Now you can check number of unique safely\n",
        "unique_counts = df_clean.nunique()\n",
        "print(unique_counts)\n"
      ],
      "metadata": {
        "id": "Kdq4y51jlkGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Similarity - for text 'overview' and 'final_name' - to understand words that effect popularity\n",
        "# Tow new columns: 'final_name_score' and 'overview_score'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Parameters\n",
        "TOP_N = 10000\n",
        "BATCH_SIZE = 5000\n",
        "\n",
        "# Select Top-N by popularity\n",
        "top_df = df_final.nlargest(TOP_N, 'popularity')\n",
        "top_final_names = top_df['final_name'].fillna('').values\n",
        "top_overviews = top_df['overview'].fillna('').values\n",
        "top_popularity = top_df['popularity'].values\n",
        "\n",
        "# TF-IDF on words only\n",
        "tfidf_vectorizer_name = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "tfidf_vectorizer_overview = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "tfidf_vectorizer_name.fit(top_final_names)\n",
        "tfidf_vectorizer_overview.fit(top_overviews)\n",
        "\n",
        "# Transform Top-N\n",
        "top_final_names_tfidf = tfidf_vectorizer_name.transform(top_final_names)\n",
        "top_overviews_tfidf = tfidf_vectorizer_overview.transform(top_overviews)\n",
        "\n",
        "# A function to calculate a grade in a batch\n",
        "def compute_score_batch(text_series, tfidf_vectorizer, top_tfidf, top_popularity):\n",
        "    scores = np.zeros(len(text_series))\n",
        "    for start in range(0, len(text_series), BATCH_SIZE):\n",
        "        end = min(start + BATCH_SIZE, len(text_series))\n",
        "        batch_text = text_series[start:end].fillna('').values\n",
        "        batch_tfidf = tfidf_vectorizer.transform(batch_text)\n",
        "        cos_sim = cosine_similarity(batch_tfidf, top_tfidf)\n",
        "        scores[start:end] = np.dot(cos_sim, top_popularity) / (cos_sim.sum(axis=1) + 1e-6)\n",
        "    return scores\n",
        "\n",
        "# Calculation of grades\n",
        "df_final['final_name_score'] = compute_score_batch(df_final['final_name'], tfidf_vectorizer_name, top_final_names_tfidf, top_popularity)\n",
        "df_final['overview_score'] = compute_score_batch(df_final['overview'], tfidf_vectorizer_overview, top_overviews_tfidf, top_popularity)\n",
        "\n",
        "# test\n",
        "print(df_final[['final_name', 'final_name_score', 'overview_score', 'popularity']].head())\n"
      ],
      "metadata": {
        "id": "xWpInPw8oDV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New df: df_final"
      ],
      "metadata": {
        "id": "ARTnLcrqY8B9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df_final.notnull().sum(),\n",
        "    'null_percent': df_final.isnull().mean() * 100,\n",
        "    'num_unique': df_final.nunique(),\n",
        "    'data_type': df_final.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "iHwO7kKQBObF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head()"
      ],
      "metadata": {
        "id": "UK9auephDVU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.describe()\n"
      ],
      "metadata": {
        "id": "OBDwYP_6EdRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continue correlation and statistics"
      ],
      "metadata": {
        "id": "adJjV4vSZEd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check statistics related to 'popularity'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, f_oneway, pearsonr\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "target = 'popularity'\n",
        "results = []\n",
        "\n",
        "for col in df_final.columns:\n",
        "    if col == target:\n",
        "        continue\n",
        "\n",
        "    # 1ï¸âƒ£ boolean columns â†’ t-test\n",
        "    if df_final[col].dtype == 'bool':\n",
        "        groups = df_final[col].unique()\n",
        "        if len(groups) == 2:\n",
        "            group1 = df_final[df_final[col] == groups[0]][target]\n",
        "            group2 = df_final[df_final[col] == groups[1]][target]\n",
        "            if len(group1) > 1 and len(group2) > 1:\n",
        "                stat, pval = ttest_ind(group1, group2, equal_var=False, nan_policy='omit')\n",
        "                results.append({'feature': col, 'test': 't-test', 'p_value': pval})\n",
        "\n",
        "    # 2ï¸âƒ£  Categorical columns â†’ ANOVA limited to â‰¤10 categories\n",
        "    elif df_final[col].dtype == 'object' or str(df_final[col].dtype).startswith('category'):\n",
        "        n_unique = df_final[col].nunique()\n",
        "        if 1 < n_unique <= 10:\n",
        "            groups = [df_final[df_final[col] == val][target]\n",
        "                      for val in df_final[col].unique()\n",
        "                      if len(df_final[df_final[col] == val]) > 1]\n",
        "            if len(groups) > 1:\n",
        "                stat, pval = f_oneway(*groups)\n",
        "                results.append({'feature': col, 'test': 'ANOVA', 'p_value': pval})\n",
        "\n",
        "    # 3ï¸âƒ£ Numeric columns â†’ Pearson correlation\n",
        "    elif is_numeric_dtype(df_final[col]):\n",
        "        if df_final[col].nunique() > 1:\n",
        "            corr, pval = pearsonr(df_final[col], df_final[target])\n",
        "            results.append({'feature': col, 'test': 'Correlation', 'p_value': pval, 'corr': corr})\n",
        "\n",
        "# Create summary table\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df['significant'] = results_df['p_value'] < 0.05\n",
        "\n",
        "# Sort by significance\n",
        "results_df = results_df.sort_values('p_value')\n",
        "\n",
        "# Display sorted results\n",
        "pd.set_option('display.float_format', '{:.3e}'.format)\n",
        "display(results_df)\n"
      ],
      "metadata": {
        "id": "ci5ITOiBxx0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check statistic related to 'popularity'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Select all numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Calculate correlation with target\n",
        "corr_matrix = df[numeric_cols].corr()\n",
        "pop_corr = corr_matrix['popularity'].drop('popularity')\n",
        "\n",
        "# Create DataFrame sorted by absolute value of correlation\n",
        "corr_df = pd.DataFrame({\n",
        "    'feature': pop_corr.index,\n",
        "    'corr': pop_corr.values\n",
        "}).sort_values(by='corr', key=abs, ascending=False)\n",
        "\n",
        "# Add a column that defines color by positive/negative\n",
        "corr_df['color'] = corr_df['corr'].apply(lambda x: 'green' if x > 0 else 'red')\n",
        "\n",
        "# Bar graph\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.barplot(\n",
        "    x='corr',\n",
        "    y='feature',\n",
        "    data=corr_df,\n",
        "    hue='color',\n",
        "    dodge=False,\n",
        "    palette={'green':'green','red':'red'},\n",
        "    legend=False\n",
        ")\n",
        "plt.axvline(0, color='black', linestyle='--')\n",
        "plt.title('Correlation of All Numeric Features with Popularity')\n",
        "plt.xlabel('Pearson Correlation')\n",
        "plt.ylabel('Feature')\n",
        "\n",
        "# Add numeric value next to bar\n",
        "for index, row in corr_df.iterrows():\n",
        "    plt.text(row['corr'], index, f\"{row['corr']:.2f}\", color='black', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NhkpQJL6tDrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check statistic related to 'popularity'\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Suppose your DataFrame is called df_final and includes the column 'popularity''\n",
        "\n",
        "# 1ï¸âƒ£ Select only numeric columns\n",
        "numeric_cols = df_final.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# 2ï¸âƒ£ Calculate correlation of each numeric column with 'popularity'\n",
        "corr_dict = {}\n",
        "for col in numeric_cols:\n",
        "    if col != 'popularity':\n",
        "        corr_dict[col] = df_final[col].corr(df_final['popularity'])\n",
        "\n",
        "# 3ï¸âƒ£ Convert to DataFrame\n",
        "top_corr = pd.DataFrame({\n",
        "    'feature': list(corr_dict.keys()),\n",
        "    'corr': list(corr_dict.values())\n",
        "})\n",
        "\n",
        "# 4ï¸âƒ£ Take the 15 values Top by absolute value of correlation\n",
        "top_corr = top_corr.reindex(top_corr['corr'].abs().sort_values(ascending=False).index).head(15)\n",
        "\n",
        "# 5ï¸âƒ£ Graph\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='corr',\n",
        "    y='feature',\n",
        "    data=top_corr,\n",
        "    hue='feature',\n",
        "    dodge=False,\n",
        "    palette='coolwarm',\n",
        "    legend=False\n",
        ")\n",
        "plt.axvline(0, color='black', linestyle='--')\n",
        "plt.title('Top 15 Numeric Features Correlation with Popularity')\n",
        "plt.xlabel('Pearson Correlation')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2PvYN9bpCq_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check statistic related to 'popularity'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Copy of results_df\n",
        "stats_df = results_df.copy()\n",
        "\n",
        "# For numeric variables, also consider direction (corr)\n",
        "stats_df['effect'] = stats_df.apply(\n",
        "    lambda row: row['corr'] if row['test']=='Correlation' else np.nan, axis=1\n",
        ")\n",
        "\n",
        "#  Consider -log10(p_value) to highlight significance in the graph\n",
        "stats_df['neg_log_p'] = -np.log10(stats_df['p_value'])\n",
        "\n",
        "# Sort by significance\n",
        "stats_df = stats_df.sort_values('neg_log_p', ascending=False)\n",
        "\n",
        "# Display Table\n",
        "display(stats_df[['feature','test','effect','p_value','neg_log_p','significant']])\n",
        "\n",
        "# Graph of top 20 by significance\n",
        "top20 = stats_df.head(20)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='neg_log_p',\n",
        "    y='feature',\n",
        "    data=top20,\n",
        "    hue='effect',\n",
        "    dodge=False,\n",
        "    palette='coolwarm',\n",
        "    legend=False\n",
        ")\n",
        "plt.axvline(-np.log10(0.05), color='black', linestyle='--', label='p=0.05')\n",
        "plt.xlabel('-log10(p-value)')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 20 Features by Significance for Popularity')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lGcYSBoOzrOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, f_oneway, pearsonr, chi2_contingency\n",
        "from pandas.api.types import is_numeric_dtype, is_bool_dtype\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "target = 'popularity'\n",
        "\n"
      ],
      "metadata": {
        "id": "8X6jy4Kq3tZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# statistics and correlation\n",
        "\n",
        "results_regular = []\n",
        "\n",
        "for col in df_final.columns:\n",
        "    if col == target:\n",
        "        continue\n",
        "\n",
        "    # Boolean â†’ t-test\n",
        "    if is_bool_dtype(df_final[col]):\n",
        "        groups = df_final[col].unique()\n",
        "        if len(groups) == 2:\n",
        "            g1 = df_final[df_final[col] == groups[0]][target]\n",
        "            g2 = df_final[df_final[col] == groups[1]][target]\n",
        "            if len(g1) > 1 and len(g2) > 1:\n",
        "                stat, pval = ttest_ind(g1, g2, equal_var=False, nan_policy='omit')\n",
        "                results_regular.append({'feature': col, 'test':'t-test', 'p_value':pval, 'effect_size':np.nan})\n",
        "\n",
        "    # small categoricals â†’ ANOVA (â‰¤10 values)\n",
        "    elif df_final[col].dtype == 'object' or str(df_final[col].dtype).startswith('category'):\n",
        "        n_unique = df_final[col].nunique()\n",
        "        if 1 < n_unique <= 10:\n",
        "            groups = [df_final[df_final[col] == val][target]\n",
        "                      for val in df_final[col].unique()\n",
        "                      if len(df_final[df_final[col] == val]) > 1]\n",
        "            if len(groups) > 1:\n",
        "                stat, pval = f_oneway(*groups)\n",
        "                results_regular.append({'feature': col, 'test':'ANOVA', 'p_value':pval, 'effect_size':np.nan})\n",
        "\n",
        "    #  Pearson correlation\n",
        "    elif is_numeric_dtype(df_final[col]):\n",
        "        if df_final[col].nunique() > 1:\n",
        "            corr, pval = pearsonr(df_final[col], df_final[target])\n",
        "            results_regular.append({'feature': col, 'test':'Correlation', 'p_value':pval, 'effect_size':corr})\n",
        "\n",
        "df_regular = pd.DataFrame(results_regular)\n",
        "df_regular['significant'] = df_regular['p_value'] < 0.05\n",
        "epsilon = 1e-300\n",
        "df_regular['neg_log_p'] = -np.log10(df_regular['p_value'].clip(lower=epsilon))\n",
        "df_regular = df_regular.sort_values('neg_log_p', ascending=False)\n"
      ],
      "metadata": {
        "id": "bXDH5zFd3w6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cramers_v(x, y):\n",
        "    \"\"\"×—×™×©×•×‘ CramÃ©r's V ×œ××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×\"\"\"\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2/n\n",
        "    r,k = confusion_matrix.shape\n",
        "    return np.sqrt(phi2 / min(k-1, r-1))\n",
        "\n",
        "large_cats = [col for col in df_final.columns\n",
        "              if (df_final[col].dtype=='object' or str(df_final[col].dtype).startswith('category'))\n",
        "              and df_final[col].nunique() > 10]\n",
        "\n",
        "results_large = []\n",
        "\n",
        "for col in large_cats:\n",
        "    try:\n",
        "        # Use qcut to divide target into 10 groups\n",
        "        cv = cramers_v(df_final[col], pd.qcut(df_final[target], q=10, duplicates='drop'))\n",
        "        results_large.append({'feature':col,'Cramers_V':cv})\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "df_large = pd.DataFrame(results_large)\n",
        "df_large = df_large.sort_values('Cramers_V', ascending=False)\n"
      ],
      "metadata": {
        "id": "QjRlwYMk3yo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show for popularity the top 20 Regular Features by Significance\n",
        "top20_regular = df_regular.head(20)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='neg_log_p',\n",
        "    y='feature',\n",
        "    data=top20_regular,\n",
        "    hue='effect_size',\n",
        "    dodge=False,\n",
        "    palette='coolwarm',\n",
        "    legend=False\n",
        ")\n",
        "plt.axvline(-np.log10(0.05), color='black', linestyle='--', label='p=0.05')\n",
        "plt.xlabel('-log10(p-value)')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 20 Regular Features by Significance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5PeIOLTl3z8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the columns above\n",
        "\n",
        "# Select the columns that summarize the graph\n",
        "\n",
        "summary_table = df_regular[['feature','test','effect_size','p_value','neg_log_p','significant']]\n",
        "\n",
        "# Sort by significance -log10(p-value)\n",
        "summary_table = summary_table.sort_values('neg_log_p', ascending=False)\n",
        "\n",
        "# Display the table\n",
        "display(summary_table)\n"
      ],
      "metadata": {
        "id": "IduJT3iZ5XzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top20_large = df_large.head(20)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='Cramers_V',\n",
        "    y='feature',\n",
        "    data=top20_large,\n",
        "    palette='viridis',\n",
        "    hue='feature',\n",
        "    dodge=False,\n",
        "    legend=False\n",
        ")\n",
        "plt.xlabel(\"CramÃ©r's V\")\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 20 Large Categorical Features by CramÃ©r\\'s V')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZbQQ5iCe8GEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of two different types of statistical analyses\n",
        "\n",
        "# First we will take care of regular variables (already have df_regular)\n",
        "\n",
        "df_regular_summary = df_regular.copy()\n",
        "df_regular_summary.rename(columns={'effect_size':'measure'}, inplace=True)\n",
        "df_regular_summary['type'] = 'regular'  # ×œ×¡××Ÿ ×¡×•×’ ××©×ª× ×”\n",
        "\n",
        "# Now large variables (CramÃ©râ€™s V)\n",
        "df_large_summary = df_large.copy()\n",
        "df_large_summary.rename(columns={'Cramers_V':'measure'}, inplace=True)\n",
        "df_large_summary['p_value'] = np.nan\n",
        "df_large_summary['neg_log_p'] = np.nan\n",
        "df_large_summary['significant'] = np.nan\n",
        "df_large_summary['test'] = 'Cramers_V'\n",
        "df_large_summary['type'] = 'large_categorical'\n",
        "\n",
        "# Adjust columns to merge\n",
        "df_large_summary = df_large_summary[['feature','test','measure','p_value','neg_log_p','significant','type']]\n",
        "\n",
        "df_regular_summary = df_regular_summary[['feature','test','measure','p_value','neg_log_p','significant','type']]\n",
        "\n",
        "# Merge the two dataframes\n",
        "df_summary_all = pd.concat([df_regular_summary, df_large_summary], ignore_index=True)\n",
        "\n",
        "# Sort by measure or neg_log_p (for regular variables)\n",
        "df_summary_all.sort_values(by=['type','neg_log_p','measure'], ascending=[True, False, False], inplace=True)\n",
        "\n",
        "# Display The table\n",
        "display(df_summary_all)\n"
      ],
      "metadata": {
        "id": "CgvgZvkx7uZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving DataFrame as CSV file\n",
        "\n",
        "df_final.to_csv('df_final_last_GitHub.csv', index=False)\n",
        "print(\"Saved df_final to 'df_final_last_GitHub.csv'\")"
      ],
      "metadata": {
        "id": "gm4kmZdmEYNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('df_final_last_GitHub.csv')"
      ],
      "metadata": {
        "id": "8zKC-_nXEyak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For GitHub upload\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KRiy918TIa5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "# Path ×œ-notebook ×”× ×•×›×—×™\n",
        "path = '/content/drive/MyDrive/Project TV show popularity/advance project/More advanced project/For GitHub/Upload to GitHub/GitHub_2_TV_show_popularity_part_two_upload.ipynb'  # ×©× ×” ×œ×¤×™ ×”× ×ª×™×‘ ×©×œ×š\n",
        "\n",
        "\n",
        "# ×§×¨×™××” ×•×¢×¨×™×›×” ×©×œ ×”-notebook\n",
        "nb = nbformat.read(path, as_version=4)\n",
        "\n",
        "# × ×™×§×•×™ metadata ×‘×¢×™×™×ª×™×ª\n",
        "if \"widgets\" in nb.metadata:\n",
        "    del nb.metadata[\"widgets\"]\n",
        "if \"colab\" in nb.metadata:\n",
        "    del nb.metadata[\"colab\"]\n",
        "if \"celltoolbar\" in nb.metadata:\n",
        "    del nb.metadata[\"celltoolbar\"]\n",
        "\n",
        "# ×©××™×¨×” ××—×“×©\n",
        "nbformat.write(nb, path)\n",
        "print(\"âœ… Notebook cleaned and ready for GitHub!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln5G5e3bIe88",
        "outputId": "1cec7e06-daa4-4e77-f2b0-095c024d4157"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Notebook cleaned and ready for GitHub!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}