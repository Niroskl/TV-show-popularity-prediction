{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Third notebook - upload file df_final_last_GitHub"
      ],
      "metadata": {
        "id": "dbJ456smJZ9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4rkBKkInTh2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('df_final_last_GitHub.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Rwhdn6XS6znM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start with ydata-profiling report - to have a good overview of all data set\n"
      ],
      "metadata": {
        "id": "fDBU5w_c43-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install ydata-profiling --quiet\n",
        "\n",
        "from ydata_profiling import ProfileReport\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "AVUGRnjdex6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# create report\n",
        "profile = ProfileReport(df, title=\"EDA Report - Dataset Overview\", explorative=True)\n",
        "\n",
        "profile.to_notebook_iframe()\n",
        "\n",
        "# Save HTML file\n",
        "profile.to_file(\"EDA_report.html\")\n",
        "\n",
        "print(\"✅ דו\\\"ח EDA נוצר ונשמר כ-'EDA_report.html'\")\n"
      ],
      "metadata": {
        "id": "knIcfC5AhDA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile.to_file(\"EDA_report.html\")\n"
      ],
      "metadata": {
        "id": "WN7Khj-wktlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "files.download(\"EDA_report.html\")\n"
      ],
      "metadata": {
        "id": "aJphXDy7kv4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "588P2CqYR6TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore target - popularity"
      ],
      "metadata": {
        "id": "XwTkMXdg6d-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See 'popularity' values\n",
        "\n",
        "\n",
        "unique_values = df['popularity'].unique()\n",
        "\n",
        "# how many values\n",
        "print(\"Number of unique values:\", len(unique_values))\n",
        "print(unique_values)\n",
        "\n",
        "# five first values\n",
        "print(unique_values[:5])\n",
        "\n",
        "# amount-count\n",
        "print(df['popularity'].value_counts())\n"
      ],
      "metadata": {
        "id": "3KRjxhO8o3p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lowest 5 values\n",
        "lowest_5 = np.sort(df['popularity'].unique())[:5]\n",
        "print(lowest_5)"
      ],
      "metadata": {
        "id": "4_PUHyWIpjko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['popularity'].describe()"
      ],
      "metadata": {
        "id": "4KmyAW6RBPfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See 'popularity' as a Log transformation\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# transformation-log\n",
        "# -----------------------------\n",
        "df['popularity_log'] = np.log1p(df['popularity'])\n",
        "\n",
        "# -----------------------------\n",
        "# Boxplot\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(14, 4))\n",
        "sns.boxplot(x='popularity_log', data=df, color='#66c2a5')\n",
        "plt.title(\"Distribution of log(Popularity) - Before Categorization\")\n",
        "plt.xlabel(\"log(1 + Popularity)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Histogram\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(16, 5))\n",
        "sns.histplot(df['popularity_log'], bins=30, kde=True, color='#66c2a5')\n",
        "plt.title(\"Histogram of log(Popularity) - Before Categorization\")\n",
        "plt.xlabel(\"log(1 + Popularity)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "leOG1QWA65rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divide 'popularity' into three main categories - low, med and hit - this will be the target analysis"
      ],
      "metadata": {
        "id": "HoriK61a5Y1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide 'popularity' into three main categories - low, med and hit\n",
        "# Moreover - balance the category to fit ML models predication\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# -----------------------------\n",
        "# create three categories\n",
        "# -----------------------------\n",
        "labels = ['Low', 'Mid', 'Hit']\n",
        "df['popularity_3cat'] = pd.qcut(df['popularity'], q=3, labels=labels)\n",
        "\n",
        "# -----------------------------\n",
        "# check\n",
        "# -----------------------------\n",
        "print(\"Initial distribution per category:\")\n",
        "print(df['popularity_3cat'].value_counts())\n",
        "print(\"\\nAverage popularity per category:\")\n",
        "print(df.groupby('popularity_3cat')['popularity'].mean())\n",
        "\n",
        "# -----------------------------\n",
        "# create graphs\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='popularity_3cat', y='popularity', data=df, palette='Set2')\n",
        "plt.title(\"Popularity Distribution per Category (Before Balancing)\")\n",
        "plt.xlabel(\"Popularity Category\")\n",
        "plt.ylabel(\"Popularity\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='popularity_3cat', data=df, palette='Set2')\n",
        "plt.title(\"Number of Samples per Category (Before Balancing)\")\n",
        "plt.xlabel(\"Popularity Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Split by categories\n",
        "# -----------------------------\n",
        "df_low = df[df['popularity_3cat'] == 'Low']\n",
        "df_mid = df[df['popularity_3cat'] == 'Mid']\n",
        "df_hit = df[df['popularity_3cat'] == 'Hit']\n",
        "\n",
        "# -----------------------------\n",
        "# Increase the Mid category to the size of the largest category\n",
        "# -----------------------------\n",
        "target_count = max(len(df_low), len(df_hit))\n",
        "df_mid_upsampled = resample(\n",
        "    df_mid,\n",
        "    replace=True,       # מאפשר שכפול\n",
        "    n_samples=target_count,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# all data\n",
        "# -----------------------------\n",
        "df_balanced = pd.concat([df_low, df_mid_upsampled, df_hit])\n",
        "\n",
        "# -----------------------------\n",
        "# Check after balancing\n",
        "# -----------------------------\n",
        "print(\"\\nBalanced distribution per category:\")\n",
        "print(df_balanced['popularity_3cat'].value_counts())\n",
        "\n",
        "print(\"\\nAverage popularity per category after balancing:\")\n",
        "print(df_balanced.groupby('popularity_3cat')['popularity'].mean())\n",
        "\n",
        "# -----------------------------\n",
        "# graphs after balancing\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='popularity_3cat', y='popularity', data=df_balanced, palette='cool')\n",
        "plt.title(\"Popularity Distribution per Category (After Balancing)\")\n",
        "plt.xlabel(\"Popularity Category\")\n",
        "plt.ylabel(\"Popularity\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='popularity_3cat', data=df_balanced, palette='cool')\n",
        "plt.title(\"Number of Samples per Category (After Balancing)\")\n",
        "plt.xlabel(\"Popularity Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6e82C67L8LKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More figures for the transformation\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# create three categories\n",
        "# -----------------------------\n",
        "labels = ['Low', 'Mid', 'Hit']\n",
        "df['popularity_3cat'] = pd.qcut(df['popularity'], q=3, labels=labels)\n",
        "\n",
        "# -----------------------------\n",
        "# transformation - log\n",
        "# -----------------------------\n",
        "df['popularity_log'] = np.log1p(df['popularity'])  # log(1 + popularity)\n",
        "\n",
        "# -----------------------------\n",
        "# Split by categories\n",
        "# -----------------------------\n",
        "df_low = df[df['popularity_3cat'] == 'Low']\n",
        "df_mid = df[df['popularity_3cat'] == 'Mid']\n",
        "df_hit = df[df['popularity_3cat'] == 'Hit']\n",
        "\n",
        "# -----------------------------\n",
        "# Increase the Mid category to the size of the largest category\n",
        "# -----------------------------\n",
        "target_count = max(len(df_low), len(df_hit))\n",
        "df_mid_upsampled = resample(\n",
        "    df_mid,\n",
        "    replace=True,\n",
        "    n_samples=target_count,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Connecting all the data\n",
        "# -----------------------------\n",
        "df_balanced = pd.concat([df_low, df_mid_upsampled, df_hit])\n",
        "\n",
        "# -----------------------------\n",
        "# Graphs after log transformation and balancing\n",
        "# -----------------------------\n",
        "\n",
        "# Boxplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(y='popularity_3cat', x='popularity_log', data=df_balanced, palette='viridis')\n",
        "plt.title(\"Log(Popularity) Distribution per Category (After Balancing)\")\n",
        "plt.ylabel(\"Popularity Category\")\n",
        "plt.xlabel(\"log(1 + Popularity)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Barplot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(\n",
        "    y='popularity_3cat',\n",
        "    x='popularity_log',\n",
        "    data=df_balanced,\n",
        "    estimator=np.mean,\n",
        "    ci='sd',\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.title(\"Mean log(Popularity) per Category (After Balancing)\")\n",
        "plt.ylabel(\"Popularity Category\")\n",
        "plt.xlabel(\"Mean log(1 + Popularity)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fj1NKSZi8Lzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "_qEqo-_8H_-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "XbcBpeomAs7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To have an overview of missing data and types of data\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'non_null_count': df.notnull().sum(),\n",
        "    'null_percent': df.isnull().mean() * 100,\n",
        "    'num_unique': df.nunique(),\n",
        "    'data_type': df.dtypes,\n",
        "}).sort_values(by='null_percent', ascending=False)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "ufd31ZZUoDUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Before continue - remove unneeded columns\n",
        "# Imputetion - Fill nulls in columns 'first_year' and 'last_year'"
      ],
      "metadata": {
        "id": "RTEjo4_p6OAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns that are not useful for modeling\n",
        "df = df.drop(columns=['final_name', 'id', 'vote_count', 'vote_average', 'type', 'overview', 'episode_run_time_outlier_iqr', 'popularity_log'])\n",
        "\n",
        "print(\"Remaining columns after dropping 'final_name' and 'id':\")\n",
        "print(df.columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "ccvdiehW8BKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputetion for 'first_year' and 'last_year' by using other relevant columns\n",
        "\n",
        "# Columns that have a logical relationship to years\n",
        "cols_for_impute = ['production_countries', 'networks', 'original_language', 'type_encoded']\n",
        "\n",
        "# -----------------------------\n",
        "# by Fill first_year\n",
        "# -----------------------------\n",
        "df['first_year'] = df.groupby(cols_for_impute)['first_year'].transform(\n",
        "    lambda x: x.fillna(x.mean())\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# by Fill last_year\n",
        "# -----------------------------\n",
        "df['last_year'] = df.groupby(cols_for_impute)['last_year'].transform(\n",
        "    lambda x: x.fillna(x.mean())\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Fallback: If there is still NaN, we will use the overall average of the column\n",
        "# -----------------------------\n",
        "df['first_year'] = df['first_year'].fillna(df['first_year'].mean())\n",
        "df['last_year'] = df['last_year'].fillna(df['last_year'].mean())\n",
        "\n",
        "# -----------------------------\n",
        "# final check\n",
        "# -----------------------------\n",
        "print(df[['first_year','last_year']].isnull().sum())\n",
        "print(df[['first_year','last_year']].describe())\n"
      ],
      "metadata": {
        "id": "RT2I210sLKKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "KwBXbH3yaQp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "RtKe7CpEgtL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before the next step - more correlation"
      ],
      "metadata": {
        "id": "6-jOQgMmfSRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a series with correlation against popularity\n",
        "correlations = df.select_dtypes(include=['number', 'bool']).corrwith(df['popularity'])\n",
        "\n",
        "# remove 'popularity'\n",
        "correlations = correlations.drop(['popularity'], errors='ignore')\n",
        "\n",
        "# Sort by correlation strength (highest to lowest)\n",
        "correlations = correlations.sort_values(ascending=False)\n",
        "\n",
        "# create DataFrame\n",
        "corr_table = correlations.reset_index()\n",
        "corr_table.columns = ['Feature', 'Correlation_with_popularity']\n",
        "\n",
        "top30 = corr_table.head(30)\n",
        "top30 = top30.iloc[::-1]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    x='Correlation_with_popularity',\n",
        "    y='Feature',\n",
        "    data=top30,\n",
        "    color='skyblue'\n",
        ")\n",
        "\n",
        "# add values\n",
        "for i, value in enumerate(top30['Correlation_with_popularity']):\n",
        "    plt.text(value + 0.002, i, f\"{value:.3f}\", va='center', fontsize=10, color='navy')\n",
        "\n",
        "plt.title(\"Top 30 Correlations with Popularity\", fontsize=14)\n",
        "plt.xlabel(\"Pearson correlation (r)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_GnceVdKf78V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Create a series with correlation against popularity\n",
        "\n",
        "correlations = df.select_dtypes(include=['number', 'bool']).corrwith(df['popularity'])\n",
        "\n",
        "# Filter both 'popularity' and 'popularity_log'\n",
        "correlations = correlations.drop(['popularity'], errors='ignore')\n",
        "\n",
        "# Keep only positive values ​​(negatives become zero)\n",
        "correlations = correlations.clip(lower=0)\n",
        "\n",
        "# Sort by correlation strength (highest to lowest)\n",
        "correlations = correlations.sort_values(ascending=False)\n",
        "\n",
        "# create DataFrame\n",
        "corr_table = correlations.reset_index()\n",
        "corr_table.columns = ['Feature', 'Correlation_with_popularity']\n",
        "\n",
        "# Horizontal bar graph for all variables\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(\n",
        "    x='Correlation_with_popularity',\n",
        "    y='Feature',\n",
        "    data=corr_table,\n",
        "    color='skyblue'\n",
        ")\n",
        "\n",
        "# Add correlation values ​​next to each bar\n",
        "for i, value in enumerate(corr_table['Correlation_with_popularity']):\n",
        "    plt.text(value + 0.002, i, f\"{value:.3f}\", va='center', fontsize=10, color='navy')\n",
        "\n",
        "plt.title(\"Correlations with Popularity (Negative set to 0) – All Features\", fontsize=16)\n",
        "plt.xlabel(\"Pearson correlation (r)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SbV-eywxhM5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =========================\n",
        "# Creating new features\n",
        "# =========================\n",
        "df['vote_count_x_vote_avg'] = df['vote_count_group'] * df['vote_average_group']\n",
        "df['episodes_per_season'] = df['number_of_episodes_clean'] / (df['number_of_seasons_clean'] + 1)\n",
        "df['age'] = df['last_year'] - df['first_year']\n",
        "\n",
        "# Handling infinite or missing values\n",
        "for col in ['vote_count_x_vote_avg', 'episodes_per_season', 'age']:\n",
        "    df[col] = df[col].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "# =========================\n",
        "# Calculating correlation against popularity\n",
        "# =========================\n",
        "numeric_cols = df.select_dtypes(include=['number', 'bool']).columns.drop(['popularity'], errors='ignore')\n",
        "correlations = df[numeric_cols].corrwith(df['popularity'])\n",
        "\n",
        "# Negative values ​​become zero\n",
        "correlations = correlations.clip(lower=0)\n",
        "\n",
        "# Sort by correlation strength (highest to lowest)\n",
        "correlations = correlations.sort_values(ascending=False)\n",
        "\n",
        "# Create DataFrame\n",
        "corr_table = correlations.reset_index()\n",
        "corr_table.columns = ['Feature', 'Correlation_with_popularity']\n",
        "\n",
        "# =========================\n",
        "# Horizontal bar graph\n",
        "# =========================\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(\n",
        "    x='Correlation_with_popularity',\n",
        "    y='Feature',\n",
        "    data=corr_table,\n",
        "    color='skyblue'\n",
        ")\n",
        "\n",
        "# Adding correlation values ​​next to each bar\n",
        "for i, value in enumerate(corr_table['Correlation_with_popularity']):\n",
        "    plt.text(value + 0.002, i, f\"{value:.3f}\", va='center', fontsize=10, color='navy')\n",
        "\n",
        "plt.title(\"Correlations with Popularity (Negative set to 0) – All Features\", fontsize=16)\n",
        "plt.xlabel(\"Pearson correlation (r)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "98OxSbugkgTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection"
      ],
      "metadata": {
        "id": "4-dKgfin7MnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# -----------------------------\n",
        "# Fast sampling if many rows\n",
        "# -----------------------------\n",
        "max_rows = 30000\n",
        "if df.shape[0] > max_rows:\n",
        "    df_sample = df.sample(max_rows, random_state=42).copy()\n",
        "else:\n",
        "    df_sample = df.copy()\n",
        "\n",
        "# -----------------------------\n",
        "# Features (all columns except popularity-related)\n",
        "# -----------------------------\n",
        "exclude_cols = ['popularity', 'popularity_bin', 'popularity_bin_equal', 'popularity_3cat']\n",
        "feature_columns = [c for c in df_sample.columns if c not in exclude_cols]\n",
        "X = df_sample[feature_columns]\n",
        "y = df_sample['popularity_3cat']\n",
        "\n",
        "# -----------------------------\n",
        "# Subset features if more than 100\n",
        "# -----------------------------\n",
        "max_features = 100\n",
        "if X.shape[1] > max_features:\n",
        "    X_fast = X.iloc[:, :max_features].copy()\n",
        "else:\n",
        "    X_fast = X.copy()\n",
        "y_fast = y.copy()\n",
        "\n",
        "# -----------------------------\n",
        "# Fill missing values עם mean\n",
        "# -----------------------------\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_fast = pd.DataFrame(imputer.fit_transform(X_fast), columns=X_fast.columns)\n",
        "\n",
        "# -----------------------------\n",
        "# Fast linear models\n",
        "# -----------------------------\n",
        "logreg_l1 = LogisticRegression(penalty='l1', solver='saga', max_iter=500).fit(X_fast, y_fast)\n",
        "logreg_l2 = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=500).fit(X_fast, y_fast)\n",
        "svc = LinearSVC(max_iter=500).fit(X_fast, y_fast)\n",
        "\n",
        "# -----------------------------\n",
        "# fast Tree-based models\n",
        "# -----------------------------\n",
        "rf = RandomForestClassifier(n_estimators=10, max_depth=3, n_jobs=-1).fit(X_fast, y_fast)\n",
        "gb = GradientBoostingClassifier(n_estimators=10, max_depth=2).fit(X_fast, y_fast)\n",
        "\n",
        "# -----------------------------\n",
        "# Feature selection flags\n",
        "# -----------------------------\n",
        "logreg_l1_selected = (np.abs(logreg_l1.coef_).sum(axis=0) > 0).astype(int)\n",
        "logreg_l2_selected = (np.abs(logreg_l2.coef_).sum(axis=0) > 0).astype(int)\n",
        "svc_selected = (np.abs(svc.coef_).sum(axis=0) > 0).astype(int)\n",
        "rf_selected = (rf.feature_importances_ > 0).astype(int)\n",
        "gb_selected = (gb.feature_importances_ > 0).astype(int)\n",
        "\n",
        "# -----------------------------\n",
        "# Summary DataFrame\n",
        "# -----------------------------\n",
        "selection_df = pd.DataFrame({\n",
        "    'Feature': X_fast.columns,\n",
        "    'Logistic_L1': logreg_l1_selected,\n",
        "    'Logistic_L2': logreg_l2_selected,\n",
        "    'LinearSVC': svc_selected,\n",
        "    'RandomForest': rf_selected,\n",
        "    'GradientBoost': gb_selected\n",
        "})\n",
        "\n",
        "selection_df['Sum'] = selection_df[['Logistic_L1','Logistic_L2','LinearSVC','RandomForest','GradientBoost']].sum(axis=1)\n",
        "selection_df = selection_df.sort_values(by='Sum', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(selection_df)\n"
      ],
      "metadata": {
        "id": "yPkIMf4n7S7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection"
      ],
      "metadata": {
        "id": "CVRVjdmtir0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple models run - no feature engineering\n",
        "# =====================================\n",
        "# Simple Models Run + Summary Table + Visualization\n",
        "# =====================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, log_loss, roc_auc_score\n",
        "\n",
        "# ===============================\n",
        "# Creating popular categories\n",
        "# ===============================\n",
        "labels = ['Low', 'Mid', 'Hit']\n",
        "df['popularity_3cat'] = pd.qcut(df['popularity'], q=3, labels=labels)\n",
        "\n",
        "# ===============================\n",
        "# Features & target\n",
        "# ===============================\n",
        "exclude_cols = ['popularity', 'popularity_bin', 'popularity_bin_equal', 'popularity_3cat']\n",
        "feature_columns = [c for c in df.columns if c not in exclude_cols]\n",
        "X = df[feature_columns]\n",
        "y = df['popularity_3cat']\n",
        "\n",
        "# Fill missing numeric values only\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].mean())\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# Define classifiers\n",
        "# ===============================\n",
        "models = {\n",
        "    'Logistic_L1': LogisticRegression(penalty='l1', solver='saga', max_iter=500, class_weight='balanced'),\n",
        "    'Logistic_L2': LogisticRegression(penalty='l2', solver='lbfgs', max_iter=500, class_weight='balanced'),\n",
        "    'LinearSVC': LinearSVC(max_iter=500, class_weight='balanced'),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=5, n_jobs=-1, random_state=42, class_weight='balanced'),\n",
        "    'GradientBoost': GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        num_class=len(le.classes_),\n",
        "        n_estimators=100,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.1,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# ===============================\n",
        "# Train models & evaluate\n",
        "# ===============================\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    #  XGBoost, AdaBoost, GradientBoost  predict_proba\n",
        "    try:\n",
        "        y_proba = model.predict_proba(X_test)\n",
        "        logloss = log_loss(y_test, y_proba)\n",
        "        auc = roc_auc_score(pd.get_dummies(y_test), y_proba, multi_class='ovr')\n",
        "    except:\n",
        "        #  LinearSVC  predict_proba\n",
        "        logloss = np.nan\n",
        "        auc = np.nan\n",
        "\n",
        "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': round(report['accuracy'], 3),\n",
        "        'Precision': round(report['macro avg']['precision'], 3),\n",
        "        'Recall': round(report['macro avg']['recall'], 3),\n",
        "        'F1-score': round(report['macro avg']['f1-score'], 3),\n",
        "        'Log-loss': None if np.isnan(logloss) else round(logloss, 3),\n",
        "        'AUC': None if np.isnan(auc) else round(auc, 3)\n",
        "    })\n",
        "\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Accuracy:\", report['accuracy'])\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# ===============================\n",
        "# Summary Table\n",
        "# ===============================\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=== Summary Table ===\")\n",
        "print(results_df)\n",
        "\n",
        "# ===============================\n",
        "# Visualization\n",
        "# ===============================\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "results_melted = results_df.melt(id_vars='Model', value_vars=metrics_to_plot,\n",
        "                                 var_name='Metric', value_name='Score')\n",
        "sns.barplot(data=results_melted, x='Metric', y='Score', hue='Model')\n",
        "plt.title('Model Performance Comparison (Simple Models)', fontsize=14, weight='bold')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PiebBgwfomOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# models with feature engineering"
      ],
      "metadata": {
        "id": "DwVbxYfsB6c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A bit improved model - with feature engineering\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ===============================\n",
        "# Feature Engineering\n",
        "# ===============================\n",
        "df['vote_count_x_vote_avg'] = df['vote_count_group'] * df['vote_average_group']\n",
        "df['episodes_per_season'] = df['number_of_episodes_clean'] / (df['number_of_seasons_clean'] + 1)\n",
        "df['vote_count_x_vote_avg'] = df['vote_count_x_vote_avg'].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "df['episodes_per_season'] = df['episodes_per_season'].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "# ===============================\n",
        "# Creating popular categories\n",
        "# ===============================\n",
        "labels = ['Low', 'Mid', 'Hit']\n",
        "df['popularity_3cat'] = pd.qcut(df['popularity'], q=3, labels=labels)\n",
        "\n",
        "# ===============================\n",
        "# Mid category balance\n",
        "# ===============================\n",
        "df_low = df[df['popularity_3cat'] == 'Low']\n",
        "df_mid = df[df['popularity_3cat'] == 'Mid']\n",
        "df_hit = df[df['popularity_3cat'] == 'Hit']\n",
        "\n",
        "target_count = max(len(df_low), len(df_hit))\n",
        "df_mid_upsampled = resample(df_mid, replace=True, n_samples=target_count, random_state=42)\n",
        "df_balanced = pd.concat([df_low, df_mid_upsampled, df_hit])\n",
        "\n",
        "print(\"Balanced distribution per category:\")\n",
        "print(df_balanced['popularity_3cat'].value_counts())\n",
        "print(\"\\nAverage popularity per category after balancing:\")\n",
        "print(df_balanced.groupby('popularity_3cat')['popularity'].mean())\n",
        "\n",
        "# ===============================\n",
        "# Features & target\n",
        "# ===============================\n",
        "exclude_cols = ['popularity', 'popularity_bin', 'popularity_bin_equal', 'popularity_3cat']\n",
        "feature_columns = [c for c in df_balanced.columns if c not in exclude_cols]\n",
        "X = df_balanced[feature_columns]\n",
        "y = df_balanced['popularity_3cat']\n",
        "\n",
        "# Fill missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# ===============================\n",
        "# Define classifiers\n",
        "# ===============================\n",
        "models = {\n",
        "    'Logistic_L1': LogisticRegression(penalty='l1', solver='saga', max_iter=500, class_weight='balanced'),\n",
        "    'Logistic_L2': LogisticRegression(penalty='l2', solver='lbfgs', max_iter=500, class_weight='balanced'),\n",
        "    'LinearSVC': LinearSVC(max_iter=500, class_weight='balanced'),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=5, n_jobs=-1, random_state=42, class_weight='balanced'),\n",
        "    'GradientBoost': GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(objective='multi:softmax', num_class=len(le.classes_), n_estimators=100,\n",
        "                                 max_depth=3, learning_rate=0.1, use_label_encoder=False, eval_metric='mlogloss',\n",
        "                                 random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "# ===============================\n",
        "# Train models & evaluate\n",
        "# ===============================\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "5vYGF_32x4I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A very good model - but before Grid optimization\n",
        "# There is a better model\n",
        "#\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==============================\n",
        "# Assume df_balanced is ready (after upsampling Mid)\n",
        "# ==============================\n",
        "\n",
        "# -----------------------------\n",
        "# Features (exclude popularity-related)\n",
        "# -----------------------------\n",
        "exclude_cols = ['popularity', 'popularity_bin', 'popularity_bin_equal', 'popularity_3cat']\n",
        "feature_columns = [c for c in df_balanced.columns if c not in exclude_cols]\n",
        "X = df_balanced[feature_columns]\n",
        "y = df_balanced['popularity_3cat']\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Fill missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# RandomForest GridSearch\n",
        "# -----------------------------\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'class_weight': ['balanced'],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    rf_params,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"=== Best RandomForest ===\")\n",
        "print(\"Best params:\", rf_grid.best_params_)\n",
        "y_pred_rf = rf_grid.predict(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=le.classes_))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "# -----------------------------\n",
        "# XGBoost GridSearch\n",
        "# -----------------------------\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "xgb_grid = GridSearchCV(\n",
        "    xgb.XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=len(le.classes_),\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    xgb_params,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n=== Best XGBoost ===\")\n",
        "print(\"Best params:\", xgb_grid.best_params_)\n",
        "y_pred_xgb = xgb_grid.predict(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "ArvgCiCn4zrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance for the model above\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# best_rf\n",
        "best_rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "best_rf.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# Feature Importance\n",
        "# -----------------------------\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': best_rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# print 20 most important features\n",
        "print(feature_importances.head(20))\n",
        "\n",
        "# -----------------------------\n",
        "# graph\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20), palette='viridis')\n",
        "plt.title('Top 20 Feature Importances - RandomForest')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OzoL1a4WB_MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A model with Train / Dev / Test Split\n",
        "# ===============================\n",
        "# MODEL: Train / Dev / Test Split\n",
        "# ===============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ===============================\n",
        "# Data preparation\n",
        "# ===============================\n",
        "exclude_cols = ['popularity', 'popularity_bin', 'popularity_bin_equal', 'popularity_3cat']\n",
        "feature_columns = [c for c in df_balanced.columns if c not in exclude_cols]\n",
        "\n",
        "X = df_balanced[feature_columns]\n",
        "y = df_balanced['popularity_3cat']\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# ===============================\n",
        "# Split: Train (60%), Dev (20%), Test (20%)\n",
        "# ===============================\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
        ")  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Dev: {len(X_dev)}, Test: {len(X_test)}\")\n",
        "\n",
        "# ===============================\n",
        "# Random Forest Grid Search (on Train only)\n",
        "# ===============================\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'class_weight': ['balanced'],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    rf_params,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"=== Best RandomForest (Train) ===\")\n",
        "print(\"Best params:\", rf_grid.best_params_)\n",
        "\n",
        "# Evaluate on Dev Set\n",
        "y_pred_dev_rf = rf_grid.predict(X_dev)\n",
        "print(\"\\n--- RandomForest DEV Performance ---\")\n",
        "print(classification_report(y_dev, y_pred_dev_rf, target_names=le.classes_))\n",
        "print(\"Confusion Matrix (Dev):\")\n",
        "print(confusion_matrix(y_dev, y_pred_dev_rf))\n",
        "\n",
        "# Evaluate on Test Set\n",
        "y_pred_test_rf = rf_grid.predict(X_test)\n",
        "print(\"\\n--- RandomForest TEST Performance ---\")\n",
        "print(classification_report(y_test, y_pred_test_rf, target_names=le.classes_))\n",
        "print(\"Confusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_pred_test_rf))\n",
        "\n",
        "# ===============================\n",
        "# XGBoost Grid Search (on Train only)\n",
        "# ===============================\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "xgb_grid = GridSearchCV(\n",
        "    xgb.XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=len(le.classes_),\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    xgb_params,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n=== Best XGBoost (Train) ===\")\n",
        "print(\"Best params:\", xgb_grid.best_params_)\n",
        "\n",
        "# Evaluate on Dev Set\n",
        "y_pred_dev_xgb = xgb_grid.predict(X_dev)\n",
        "print(\"\\n--- XGBoost DEV Performance ---\")\n",
        "print(classification_report(y_dev, y_pred_dev_xgb, target_names=le.classes_))\n",
        "print(\"Confusion Matrix (Dev):\")\n",
        "print(confusion_matrix(y_dev, y_pred_dev_xgb))\n",
        "\n",
        "# Evaluate on Test Set\n",
        "y_pred_test_xgb = xgb_grid.predict(X_test)\n",
        "print(\"\\n--- XGBoost TEST Performance ---\")\n",
        "print(classification_report(y_test, y_pred_test_xgb, target_names=le.classes_))\n",
        "print(\"Confusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_pred_test_xgb))\n"
      ],
      "metadata": {
        "id": "14rNfOhEl_uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Popularity Prediction (3 categories) + Feature Engineering\n",
        "# =====================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ==============================\n",
        "# Assume df_balanced is ready\n",
        "# ==============================\n",
        "\n",
        "# ===============================\n",
        "# 1️⃣ Feature Engineering\n",
        "# ===============================\n",
        "df_balanced['vote_count_x_vote_avg'] = df_balanced['vote_count_group'] * df_balanced['vote_average_group']\n",
        "df_balanced['episodes_per_season'] = df_balanced['number_of_episodes_clean'] / (df_balanced['number_of_seasons_clean'] + 1)\n",
        "df_balanced['age'] = df_balanced['last_year'] - df_balanced['first_year']\n",
        "\n",
        "# ===============================\n",
        "# 2️⃣ Feature Selection\n",
        "# ===============================\n",
        "exclude_cols = ['popularity', 'popularity_bin', 'popularity_bin_equal', 'popularity_3cat']\n",
        "feature_columns = [c for c in df_balanced.columns if c not in exclude_cols]\n",
        "X = df_balanced[feature_columns]\n",
        "y = df_balanced['popularity_3cat']\n",
        "\n",
        "# ===============================\n",
        "# 3️⃣ Encoding & Imputation\n",
        "# ===============================\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# ===============================\n",
        "# 4️⃣ Train/Test Split\n",
        "# ===============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 5️⃣ RandomForest GridSearch\n",
        "# ===============================\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'class_weight': ['balanced'],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    rf_params,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"=== Best RandomForest ===\")\n",
        "print(\"Best params:\", rf_grid.best_params_)\n",
        "y_pred_rf = rf_grid.predict(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=le.classes_))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "# ===============================\n",
        "# 6️⃣ XGBoost GridSearch\n",
        "# ===============================\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "xgb_grid = GridSearchCV(\n",
        "    xgb.XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=len(le.classes_),\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    xgb_params,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n=== Best XGBoost ===\")\n",
        "print(\"Best params:\", xgb_grid.best_params_)\n",
        "y_pred_xgb = xgb_grid.predict(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "nNusnP_ZCk3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ===============================\n",
        "# 🎯 Feature Importance – RandomForest\n",
        "# ===============================\n",
        "best_rf = rf_grid.best_estimator_\n",
        "\n",
        "rf_importances = pd.Series(\n",
        "    best_rf.feature_importances_,\n",
        "    index=X.columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "rf_importances.head(15).plot(kind='barh')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Top 15 Feature Importances - Random Forest\", fontsize=14)\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== RandomForest Feature Importance ===\")\n",
        "print(rf_importances.head(20))\n",
        "\n",
        "# ===============================\n",
        "# 🎯 Feature Importance – XGBoost\n",
        "# ===============================\n",
        "best_xgb = xgb_grid.best_estimator_\n",
        "\n",
        "xgb_importances = pd.Series(\n",
        "    best_xgb.feature_importances_,\n",
        "    index=X.columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "xgb_importances.head(15).plot(kind='barh', color='orange')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Top 15 Feature Importances - XGBoost\", fontsize=14)\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== XGBoost Feature Importance ===\")\n",
        "print(xgb_importances.head(20))\n"
      ],
      "metadata": {
        "id": "_Kf8sNLMw5Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The best model!\n",
        "\n",
        "# =====================================\n",
        "# Popularity Prediction (3 categories) + Feature Engineering + Expanded GridSearch + Results Table\n",
        "# =====================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, log_loss, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ===============================\n",
        "# 1️⃣ Feature Engineering\n",
        "# ===============================\n",
        "df_balanced['vote_count_x_vote_avg'] = df_balanced['vote_count_group'] * df_balanced['vote_average_group']\n",
        "df_balanced['episodes_per_season'] = df_balanced['number_of_episodes_clean'] / (df_balanced['number_of_seasons_clean'] + 1)\n",
        "df_balanced['age'] = df_balanced['last_year'] - df_balanced['first_year']\n",
        "\n",
        "# ===============================\n",
        "# 2️⃣ Feature Selection\n",
        "# ===============================\n",
        "exclude_cols = ['popularity', 'popularity_bin', 'popularity_bin_equal', 'popularity_3cat']\n",
        "feature_columns = [c for c in df_balanced.columns if c not in exclude_cols]\n",
        "X = df_balanced[feature_columns]\n",
        "y = df_balanced['popularity_3cat']\n",
        "\n",
        "# ===============================\n",
        "# 3️⃣ Encoding & Imputation\n",
        "# ===============================\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# ===============================\n",
        "# 4️⃣ Train/Test Split\n",
        "# ===============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 5️⃣ RandomForest GridSearch (Expanded)\n",
        "# ===============================\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    rf_params,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"=== Best RandomForest ===\")\n",
        "print(\"Best params:\", rf_grid.best_params_)\n",
        "y_pred_rf = rf_grid.predict(X_test)\n",
        "y_proba_rf = rf_grid.predict_proba(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=le.classes_))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "# ===============================\n",
        "# 6️⃣ XGBoost GridSearch (Expanded)\n",
        "# ===============================\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "    'gamma': [0, 1]\n",
        "}\n",
        "\n",
        "xgb_grid = GridSearchCV(\n",
        "    xgb.XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        num_class=len(le.classes_),\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    xgb_params,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n=== Best XGBoost ===\")\n",
        "print(\"Best params:\", xgb_grid.best_params_)\n",
        "y_pred_xgb = xgb_grid.predict(X_test)\n",
        "y_proba_xgb = xgb_grid.predict_proba(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb))\n",
        "\n",
        "# ===============================\n",
        "# 7️⃣ Summary Table (Results)\n",
        "# ===============================\n",
        "results = []\n",
        "\n",
        "# --- Random Forest metrics ---\n",
        "report_rf = classification_report(y_test, y_pred_rf, output_dict=True, zero_division=0)\n",
        "results.append({\n",
        "    'Model': 'RandomForest',\n",
        "    'Accuracy': round(report_rf['accuracy'], 6),\n",
        "    'Precision': round(report_rf['macro avg']['precision'], 6),\n",
        "    'Recall': round(report_rf['macro avg']['recall'], 6),\n",
        "    'f1-score': round(report_rf['macro avg']['f1-score'], 6),\n",
        "    'Log-loss': round(log_loss(y_test, y_proba_rf), 6),\n",
        "    'AUC': round(roc_auc_score(pd.get_dummies(y_test), y_proba_rf, multi_class='ovr'), 6)\n",
        "})\n",
        "\n",
        "# --- XGBoost metrics ---\n",
        "report_xgb = classification_report(y_test, y_pred_xgb, output_dict=True, zero_division=0)\n",
        "results.append({\n",
        "    'Model': 'XGBoost',\n",
        "    'Accuracy': round(report_xgb['accuracy'], 6),\n",
        "    'Precision': round(report_xgb['macro avg']['precision'], 6),\n",
        "    'Recall': round(report_xgb['macro avg']['recall'], 6),\n",
        "    'f1-score': round(report_xgb['macro avg']['f1-score'], 6),\n",
        "    'Log-loss': round(log_loss(y_test, y_proba_xgb), 6),\n",
        "    'AUC': round(roc_auc_score(pd.get_dummies(y_test), y_proba_xgb, multi_class='ovr'), 6)\n",
        "})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=== Summary Table ===\")\n",
        "print(results_df)\n",
        "\n",
        "# ===============================\n",
        "# 8️⃣ Visualization (Bar Plot)\n",
        "# ===============================\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'f1-score', 'AUC']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "results_melted = results_df.melt(id_vars='Model', value_vars=metrics_to_plot,\n",
        "                                 var_name='Metric', value_name='Score')\n",
        "sns.barplot(data=results_melted, x='Metric', y='Score', hue='Model')\n",
        "plt.title('Model Performance Comparison', fontsize=14, weight='bold')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend(title='Model')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yhW4Id3Km-e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ===============================\n",
        "# 🎯 Feature Importance – RandomForest\n",
        "# ===============================\n",
        "best_rf = rf_grid.best_estimator_\n",
        "\n",
        "rf_importances = pd.Series(\n",
        "    best_rf.feature_importances_,\n",
        "    index=X.columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "rf_importances.head(20).plot(kind='barh')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Top 20 Feature Importances - Random Forest\", fontsize=14)\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== RandomForest Feature Importance ===\")\n",
        "print(rf_importances.head(20))\n",
        "\n",
        "# ===============================\n",
        "# 🎯 Feature Importance – XGBoost\n",
        "# ===============================\n",
        "best_xgb = xgb_grid.best_estimator_\n",
        "\n",
        "xgb_importances = pd.Series(\n",
        "    best_xgb.feature_importances_,\n",
        "    index=X.columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "xgb_importances.head(20).plot(kind='barh', color='orange')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Top 20 Feature Importances - XGBoost\", fontsize=14)\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== XGBoost Feature Importance ===\")\n",
        "print(xgb_importances.head(20))\n"
      ],
      "metadata": {
        "id": "ut1QIhyeFkhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For GitHub upload\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QhTvmEMpJnvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "# Path ל-notebook הנוכחי\n",
        "path = '/content/drive/MyDrive/Project TV show popularity/advance project/More advanced project/For GitHub/Upload to GitHub/GitHub_3_and_4_TV_show_popularity_part_three_and_four_upload.ipynb'  # שנה לפי הנתיב שלך\n",
        "\n",
        "\n",
        "# קריאה ועריכה של ה-notebook\n",
        "nb = nbformat.read(path, as_version=4)\n",
        "\n",
        "# ניקוי metadata בעייתית\n",
        "if \"widgets\" in nb.metadata:\n",
        "    del nb.metadata[\"widgets\"]\n",
        "if \"colab\" in nb.metadata:\n",
        "    del nb.metadata[\"colab\"]\n",
        "if \"celltoolbar\" in nb.metadata:\n",
        "    del nb.metadata[\"celltoolbar\"]\n",
        "\n",
        "# שמירה מחדש\n",
        "nbformat.write(nb, path)\n",
        "print(\"✅ Notebook cleaned and ready for GitHub!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLGOf_8gJr6o",
        "outputId": "a114b681-af3d-4eb0-f46e-4570479d3f8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Notebook cleaned and ready for GitHub!\n"
          ]
        }
      ]
    }
  ]
}